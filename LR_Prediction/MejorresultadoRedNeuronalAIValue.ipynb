{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('InterpolatedWithCAPEX2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>D REVENUE</th>\n",
       "      <th>U CR</th>\n",
       "      <th>D OE</th>\n",
       "      <th>D NOI</th>\n",
       "      <th>U CAPEX</th>\n",
       "      <th>U CWK</th>\n",
       "      <th>D FCF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>1884.372544</td>\n",
       "      <td>976.202014</td>\n",
       "      <td>475.249997</td>\n",
       "      <td>757.519678</td>\n",
       "      <td>207.477947</td>\n",
       "      <td>3600.000000</td>\n",
       "      <td>856.600959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-31</td>\n",
       "      <td>1884.566826</td>\n",
       "      <td>983.762225</td>\n",
       "      <td>485.004015</td>\n",
       "      <td>734.017979</td>\n",
       "      <td>207.303532</td>\n",
       "      <td>3638.472896</td>\n",
       "      <td>810.859727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-02-28</td>\n",
       "      <td>1884.761107</td>\n",
       "      <td>991.322435</td>\n",
       "      <td>494.758033</td>\n",
       "      <td>710.516281</td>\n",
       "      <td>207.129117</td>\n",
       "      <td>3676.945791</td>\n",
       "      <td>765.118495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-03-31</td>\n",
       "      <td>1884.955389</td>\n",
       "      <td>998.882646</td>\n",
       "      <td>504.512051</td>\n",
       "      <td>687.014582</td>\n",
       "      <td>206.954702</td>\n",
       "      <td>3715.418687</td>\n",
       "      <td>719.377263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-04-30</td>\n",
       "      <td>1880.767673</td>\n",
       "      <td>1006.377690</td>\n",
       "      <td>481.542613</td>\n",
       "      <td>511.922217</td>\n",
       "      <td>207.283705</td>\n",
       "      <td>3792.839197</td>\n",
       "      <td>732.414605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2018-08-31</td>\n",
       "      <td>1785.193900</td>\n",
       "      <td>972.274049</td>\n",
       "      <td>461.469501</td>\n",
       "      <td>414.377788</td>\n",
       "      <td>98.125058</td>\n",
       "      <td>2795.231076</td>\n",
       "      <td>733.164793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>2018-09-30</td>\n",
       "      <td>1730.706270</td>\n",
       "      <td>953.165883</td>\n",
       "      <td>488.056175</td>\n",
       "      <td>424.307526</td>\n",
       "      <td>95.323030</td>\n",
       "      <td>2858.452805</td>\n",
       "      <td>729.764211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>2018-10-31</td>\n",
       "      <td>1667.921494</td>\n",
       "      <td>856.900384</td>\n",
       "      <td>471.261535</td>\n",
       "      <td>434.874583</td>\n",
       "      <td>92.144220</td>\n",
       "      <td>2849.606546</td>\n",
       "      <td>737.713784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>2018-11-30</td>\n",
       "      <td>1605.136718</td>\n",
       "      <td>760.634885</td>\n",
       "      <td>454.466895</td>\n",
       "      <td>445.441639</td>\n",
       "      <td>88.965411</td>\n",
       "      <td>2840.760287</td>\n",
       "      <td>745.663358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>1542.351942</td>\n",
       "      <td>664.369385</td>\n",
       "      <td>437.672255</td>\n",
       "      <td>456.008696</td>\n",
       "      <td>85.786601</td>\n",
       "      <td>2831.914028</td>\n",
       "      <td>753.612931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Dates    D REVENUE         U CR        D OE       D NOI     U CAPEX  \\\n",
       "0    2009-12-31  1884.372544   976.202014  475.249997  757.519678  207.477947   \n",
       "1    2010-01-31  1884.566826   983.762225  485.004015  734.017979  207.303532   \n",
       "2    2010-02-28  1884.761107   991.322435  494.758033  710.516281  207.129117   \n",
       "3    2010-03-31  1884.955389   998.882646  504.512051  687.014582  206.954702   \n",
       "4    2010-04-30  1880.767673  1006.377690  481.542613  511.922217  207.283705   \n",
       "..          ...          ...          ...         ...         ...         ...   \n",
       "104  2018-08-31  1785.193900   972.274049  461.469501  414.377788   98.125058   \n",
       "105  2018-09-30  1730.706270   953.165883  488.056175  424.307526   95.323030   \n",
       "106  2018-10-31  1667.921494   856.900384  471.261535  434.874583   92.144220   \n",
       "107  2018-11-30  1605.136718   760.634885  454.466895  445.441639   88.965411   \n",
       "108  2018-12-31  1542.351942   664.369385  437.672255  456.008696   85.786601   \n",
       "\n",
       "           U CWK       D FCF  \n",
       "0    3600.000000  856.600959  \n",
       "1    3638.472896  810.859727  \n",
       "2    3676.945791  765.118495  \n",
       "3    3715.418687  719.377263  \n",
       "4    3792.839197  732.414605  \n",
       "..           ...         ...  \n",
       "104  2795.231076  733.164793  \n",
       "105  2858.452805  729.764211  \n",
       "106  2849.606546  737.713784  \n",
       "107  2840.760287  745.663358  \n",
       "108  2831.914028  753.612931  \n",
       "\n",
       "[109 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "753.612931\n"
     ]
    }
   ],
   "source": [
    "max = {'D REVENUE':df['D REVENUE'].max(), 'U CR':df['U CR'].max(), 'D OE':df['D OE'].max(), \n",
    "       'D NOI':df['D NOI'].max(),'U CAPEX':df['U CAPEX'].max(), 'U CWK':df['U CWK'].max()} \n",
    "min = {'D REVENUE':df['D REVENUE'].min(), 'U CR':df['U CR'].min(), 'D OE':df['D OE'].min(), \n",
    "       'D NOI':df['D NOI'].min(),'U CAPEX':df['U CAPEX'].min(), 'U CWK':df['U CWK'].min()} \n",
    "filas, columnas = df.count()-1, len(df.columns)-1\n",
    "dataset = df.values\n",
    "DFCF = dataset[filas, columnas][1]\n",
    "print(DFCF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['2009-12-31', 1884.372544, 976.2020142, 475.24999739999987,\n",
       "        757.519678, 207.4779469, 3600.0, 856.6009594000002],\n",
       "       ['2010-01-31', 1884.5668256666668, 983.7622247333335, 485.0040154,\n",
       "        734.0179793333333, 207.30353190000002, 3638.472895666667,\n",
       "        810.8597272333334],\n",
       "       ['2010-02-28', 1884.761107333333, 991.3224352666666,\n",
       "        494.75803339999993, 710.5162806666667, 207.12911689999999,\n",
       "        3676.945791333333, 765.1184950666667],\n",
       "       ['2010-03-31', 1884.955389, 998.8826458, 504.5120514, 687.014582,\n",
       "        206.9547019, 3715.418687, 719.3772629],\n",
       "       ['2010-04-30', 1880.767673333333, 1006.3776901999997,\n",
       "        481.5426128333333, 511.92221730000006, 207.28370463333331,\n",
       "        3792.8391966666654, 732.4146046],\n",
       "       ['2010-05-31', 1876.5799576666668, 1013.8727346000001,\n",
       "        458.5731742666666, 336.8298526000001, 207.61270736666665,\n",
       "        3870.2597063333333, 745.4519462999999],\n",
       "       ['2010-06-30', 1872.392242, 1021.367779, 435.6037357, 161.7374879,\n",
       "        207.9417101, 3947.680216, 758.489288],\n",
       "       ['2010-07-31', 1853.240114, 1015.1698886666666, 421.8261523,\n",
       "        187.6358415, 207.7144804333333, 3965.748594666666,\n",
       "        756.6899263333332],\n",
       "       ['2010-08-31', 1834.087986, 1008.9719983333333, 408.0485689,\n",
       "        213.5341951, 207.48725076666665, 3983.816973333333,\n",
       "        754.8905646666667],\n",
       "       ['2010-09-30', 1814.935858, 1002.774108, 394.2709855, 239.4325487,\n",
       "        207.2600211, 4001.885352, 753.0912030000002],\n",
       "       ['2010-10-31', 1851.320774, 1008.5464486666666, 411.1460836666666,\n",
       "        246.73378476666667, 212.21753106666665, 4093.646761333333,\n",
       "        715.3372166666667],\n",
       "       ['2010-11-30', 1887.70569, 1014.3187893333335, 428.0211818333333,\n",
       "        254.03502083333333, 217.1750410333333, 4185.408170666667,\n",
       "        677.5832303333334],\n",
       "       ['2010-12-31', 1924.090606, 1020.09113, 444.89628, 261.3362569,\n",
       "        222.132551, 4277.16958, 639.829244],\n",
       "       ['2011-01-31', 1840.631611, 1004.3123142000001, 439.1857453333333,\n",
       "        253.3520595, 221.3439679, 4255.687593333333, 625.4113584666667],\n",
       "       ['2011-02-28', 1757.172616, 988.5334983999999, 433.47521066666667,\n",
       "        245.36786209999997, 220.5553848, 4234.205606666666,\n",
       "        610.9934729333334],\n",
       "       ['2011-03-31', 1673.713621, 972.7546826, 427.764676, 237.3836647,\n",
       "        219.7668017, 4212.723620000002, 596.5755874],\n",
       "       ['2011-04-30', 1697.5040236666666, 943.8024453, 430.2420961,\n",
       "        237.04147333333333, 223.6841742333333, 4253.569522333334,\n",
       "        596.6860897999999],\n",
       "       ['2011-05-31', 1721.2944263333334, 914.850208, 432.71951619999993,\n",
       "        236.69928196666663, 227.60154676666667, 4294.415424666667,\n",
       "        596.7965922000002],\n",
       "       ['2011-06-30', 1745.084829, 885.8979707000002, 435.1969363,\n",
       "        236.3570906, 231.5189193, 4335.261327, 596.9070946],\n",
       "       ['2011-07-31', 1775.998593, 875.2949231666668, 444.6121419,\n",
       "        236.2388491, 227.72962546666668, 4329.278552666667,\n",
       "        628.3878588333333],\n",
       "       ['2011-08-31', 1806.912357, 864.6918756333333, 454.02734749999996,\n",
       "        236.1206076, 223.94033163333333, 4323.295778333333,\n",
       "        659.8686230666667],\n",
       "       ['2011-09-30', 1837.826121, 854.0888281, 463.44255310000005,\n",
       "        236.0023661, 220.1510378, 4317.313004, 691.3493873],\n",
       "       ['2011-10-31', 1885.0705423333332, 857.2014145333335,\n",
       "        470.0062231666667, 244.5463731, 212.89141523333333,\n",
       "        4309.167693666666, 690.0065109333333],\n",
       "       ['2011-11-30', 1932.314963666667, 860.3140009666665,\n",
       "        476.56989323333335, 253.0903801, 205.63179266666666,\n",
       "        4301.022383333333, 688.6636345666667],\n",
       "       ['2011-12-31', 1979.559385, 863.4265874, 483.13356330000005,\n",
       "        261.6343871, 198.3721701, 4292.877073, 687.3207582],\n",
       "       ['2012-01-31', 1910.3834399999998, 858.4811889666668,\n",
       "        483.72593293333335, 263.6296255, 198.3738698333333, 4291.272806,\n",
       "        711.6253066666667],\n",
       "       ['2012-02-29', 1841.2074949999999, 853.5357905333334,\n",
       "        484.3183025666667, 265.6248639, 198.37556956666668,\n",
       "        4289.668538999998, 735.9298551333335],\n",
       "       ['2012-03-31', 1772.03155, 848.5903921, 484.9106722, 267.6201023,\n",
       "        198.3772693, 4288.064272, 760.2344036000002],\n",
       "       ['2012-04-30', 1805.3226533333332, 852.7747259666667,\n",
       "        500.10971653333337, 262.53434903333334, 203.80507153333332,\n",
       "        4248.180273333333, 760.4925157666668],\n",
       "       ['2012-05-31', 1838.6137566666666, 856.9590598333334,\n",
       "        515.3087608666667, 257.44859576666664, 209.23287376666664,\n",
       "        4208.296274666666, 760.7506279333335],\n",
       "       ['2012-06-30', 1871.90486, 861.1433937, 530.5078052, 252.3628425,\n",
       "        214.660676, 4168.412276, 761.0087401000002],\n",
       "       ['2012-07-31', 1870.0189613333332, 888.6962120999999,\n",
       "        512.9147551333333, 274.75087963333334, 210.84279596666664,\n",
       "        4100.139316, 761.8369197333334],\n",
       "       ['2012-08-31', 1868.1330626666668, 916.2490305,\n",
       "        495.32170506666665, 297.1389167666666, 207.02491593333332,\n",
       "        4031.8663560000005, 762.6650993666667],\n",
       "       ['2012-09-30', 1866.247164, 943.8018489, 477.728655, 319.5269539,\n",
       "        203.2070359, 3963.593396, 763.493279],\n",
       "       ['2012-10-31', 1872.4550559999998, 871.4747433666665,\n",
       "        485.99178150000006, 323.89898873333334, 198.78250549999998,\n",
       "        3581.357899333333, 776.451154],\n",
       "       ['2012-11-30', 1878.6629480000001, 799.1476378333333, 494.254908,\n",
       "        328.27102356666666, 194.3579751, 3199.1224026666664, 789.409029],\n",
       "       ['2012-12-31', 1884.87084, 726.8205323, 502.5180345, 332.6430584,\n",
       "        189.9334447, 2816.886906, 802.366904],\n",
       "       ['2013-01-31', 1896.30791, 753.0427803333333, 514.5447965666667,\n",
       "        333.5149285333333, 192.01279616666667, 2893.339288,\n",
       "        793.3964250333332],\n",
       "       ['2013-02-28', 1907.7449800000002, 779.2650283666666,\n",
       "        526.5715586333333, 334.38679866666666, 194.09214763333333,\n",
       "        2969.79167, 784.4259460666666],\n",
       "       ['2013-03-31', 1919.18205, 805.4872763999998, 538.5983207,\n",
       "        335.2586688, 196.1714991, 3046.244052, 775.4554671],\n",
       "       ['2013-04-30', 1867.7861953333331, 783.4079046999999,\n",
       "        530.8571651000001, 335.86746453333336, 195.6670658333333,\n",
       "        3048.921046333333, 766.0225778666667],\n",
       "       ['2013-05-31', 1816.3903406666666, 761.328533, 523.1160095,\n",
       "        336.47626026666666, 195.16263256666667, 3051.5980406666667,\n",
       "        756.5896886333333],\n",
       "       ['2013-06-30', 1764.994486, 739.2491613, 515.3748539, 337.085056,\n",
       "        194.6581993, 3054.275035, 747.1567994],\n",
       "       ['2013-07-31', 1768.6169343333331, 757.7170030999998, 520.4034373,\n",
       "        329.2451542333333, 198.2425908, 2952.306149666667, 757.3171348],\n",
       "       ['2013-08-31', 1772.2393826666666, 776.1848449, 525.4320207000002,\n",
       "        321.40525246666664, 201.8269823, 2850.337264333333,\n",
       "        767.4774702000001],\n",
       "       ['2013-09-30', 1775.861831, 794.6526867, 530.4606041000002,\n",
       "        313.5653506999999, 205.4113738, 2748.368379, 777.6378056],\n",
       "       ['2013-10-31', 1781.312525333333, 813.7964820333333,\n",
       "        531.6057520333334, 320.4643399333333, 203.1795522,\n",
       "        2446.6776800000002, 776.7867944],\n",
       "       ['2013-11-30', 1786.7632196666668, 832.9402773666667,\n",
       "        532.7508999666667, 327.36332916666663, 200.9477306, 2144.986981,\n",
       "        775.9357832000001],\n",
       "       ['2013-12-31', 1792.2139140000004, 852.0840727, 533.8960479,\n",
       "        334.2623184, 198.715909, 1843.296282, 775.0847719999998],\n",
       "       ['2014-01-31', 1772.1792736666669, 861.7296213999998, 528.3798523,\n",
       "        342.6429393, 191.0202412333333, 1839.5208699999998,\n",
       "        774.5279383333333],\n",
       "       ['2014-02-28', 1752.144633333333, 871.3751701, 522.8636567,\n",
       "        351.0235602, 183.32457346666664, 1835.7454579999999,\n",
       "        773.9711046666665],\n",
       "       ['2014-03-31', 1732.109993, 881.0207187999998, 517.3474611,\n",
       "        359.4041811, 175.6289057, 1831.970046, 773.414271],\n",
       "       ['2014-04-30', 1721.8949903333335, 890.6855609999999,\n",
       "        507.92859023333335, 349.91219946666666, 168.82089786666666,\n",
       "        1848.738663666667, 760.1008118333333],\n",
       "       ['2014-05-31', 1711.6799876666666, 900.3504032,\n",
       "        498.50971936666673, 340.4202178333333, 162.01289003333332,\n",
       "        1865.507281333333, 746.7873526666666],\n",
       "       ['2014-06-30', 1701.464985, 910.0152454, 489.09084850000005,\n",
       "        330.9282362, 155.2048822, 1882.275899, 733.4738934999998],\n",
       "       ['2014-07-31', 1728.4231946666666, 914.2662125, 481.6856930666667,\n",
       "        331.95640223333334, 150.8917052333333, 1896.6301780000001,\n",
       "        727.0572944999999],\n",
       "       ['2014-08-31', 1755.3814043333334, 918.5171796,\n",
       "        474.28053763333327, 332.98456826666666, 146.57852826666667,\n",
       "        1910.984457, 720.6406955],\n",
       "       ['2014-09-30', 1782.339614, 922.7681467, 466.8753822,\n",
       "        334.0127343000001, 142.2653513, 1925.338736, 714.2240965],\n",
       "       ['2014-10-31', 1725.137324333333, 965.4992654666668,\n",
       "        478.28002593333326, 415.6834258, 141.41725303333334,\n",
       "        1946.0452103333328, 728.1806636666665],\n",
       "       ['2014-11-30', 1667.935034666667, 1008.2303842333333,\n",
       "        489.68466966666665, 497.35411730000004, 140.56915476666666,\n",
       "        1966.751684666667, 742.1372308333333],\n",
       "       ['2014-12-31', 1610.732745, 1050.961503, 501.0893134, 579.0248088,\n",
       "        139.7210565, 1987.458159, 756.093798],\n",
       "       ['2015-01-31', 1746.422448666667, 1054.429111, 508.5803035333333,\n",
       "        579.2034107000001, 137.07026573333334, 2005.468421,\n",
       "        771.8714318333333],\n",
       "       ['2015-02-28', 1882.112152333333, 1057.8967189999998,\n",
       "        516.0712936666666, 579.3820125999998, 134.41947496666666,\n",
       "        2023.478683, 787.6490656666667],\n",
       "       ['2015-03-31', 2017.801856, 1061.364327, 523.5622838,\n",
       "        579.5606144999998, 131.7686842, 2041.488945, 803.4266995],\n",
       "       ['2015-04-30', 2107.344116, 1096.44801, 526.1169451000002,\n",
       "        577.4851386666667, 131.0318083, 2038.723757, 790.2704643666667],\n",
       "       ['2015-05-31', 2196.8863760000004, 1131.531693, 528.6716064,\n",
       "        575.4096628333333, 130.2949324, 2035.9585690000001,\n",
       "        777.1142292333334],\n",
       "       ['2015-06-30', 2286.428636, 1166.615376, 531.2262677, 573.334187,\n",
       "        129.5580565, 2033.193381, 763.9579941000002],\n",
       "       ['2015-07-31', 2528.4880129999997, 1194.063472, 562.655131,\n",
       "        551.8155191666667, 128.9129271, 2038.7990923333327, 725.2341621],\n",
       "       ['2015-08-31', 2770.5473899999997, 1221.511568, 594.0839943000002,\n",
       "        530.2968513333334, 128.2677977, 2044.4048036666666, 686.5103301],\n",
       "       ['2015-09-30', 3012.606767, 1248.959664, 625.5128576000002,\n",
       "        508.7781835, 127.6226683, 2050.010515, 647.7864981],\n",
       "       ['2015-10-31', 2903.7366663333332, 1269.6701176666666,\n",
       "        641.4096041, 508.9514627, 117.39104201, 2031.2492326666666,\n",
       "        646.7970369666667],\n",
       "       ['2015-11-30', 2794.8665656666667, 1290.3805713333334,\n",
       "        657.3063506000002, 509.1247419, 107.15941572, 2012.4879503333332,\n",
       "        645.8075758333333],\n",
       "       ['2015-12-31', 2685.996465, 1311.091025, 673.2030971, 509.2980211,\n",
       "        96.92778943, 1993.726668, 644.8181147000001],\n",
       "       ['2016-01-31', 2834.140780666666, 1366.4444733333332,\n",
       "        674.0792667333334, 500.8613563, 382.7185262866667, 2146.965891,\n",
       "        645.8685558666667],\n",
       "       ['2016-02-29', 2982.285096333333, 1421.7979216666665,\n",
       "        674.9554363666666, 492.4246915, 668.5092631433333, 2300.205114,\n",
       "        646.9189970333333],\n",
       "       ['2016-03-31', 3130.429412, 1477.1513699999996, 675.831606,\n",
       "        483.9880267, 954.3, 2453.444337, 647.9694382],\n",
       "       ['2016-04-30', 3060.8571920000004, 1508.7480799999996,\n",
       "        653.6836567666667, 473.8452801, 669.9162089666665,\n",
       "        2448.972803333333, 637.6159774666667],\n",
       "       ['2016-05-31', 2991.284972, 1540.34479, 631.5357075333333,\n",
       "        463.7025335, 385.5324179333333, 2444.5012696666668,\n",
       "        627.2625167333333],\n",
       "       ['2016-06-30', 2921.712752, 1571.9415, 609.3877583, 453.5597869,\n",
       "        101.1486269, 2440.029736, 616.909056],\n",
       "       ['2016-07-31', 2979.422658, 1553.679082333333, 612.0762081333334,\n",
       "        473.88566743333337, 103.19915363333334, 2475.9393219999997,\n",
       "        608.7162381333333],\n",
       "       ['2016-08-31', 3037.132564, 1535.416664666667, 614.7646579666666,\n",
       "        494.2115479666666, 105.24968036666668, 2511.848908,\n",
       "        600.5234202666667],\n",
       "       ['2016-09-30', 3094.84247, 1517.1542470000004, 617.4531078,\n",
       "        514.5374285, 107.3002071, 2547.758494, 592.3306024],\n",
       "       ['2016-10-31', 2941.914112333333, 1493.7506856666669,\n",
       "        601.3888980666667, 497.1393087333333, 106.27214706666666,\n",
       "        2492.278597666667, 605.3801967666667],\n",
       "       ['2016-11-30', 2788.9857546666667, 1470.347124333333,\n",
       "        585.3246883333334, 479.7411889666667, 105.24408703333332,\n",
       "        2436.798701333333, 618.4297911333333],\n",
       "       ['2016-12-31', 2636.057397, 1446.943563, 569.2604786, 462.3430692,\n",
       "        104.216027, 2381.318805, 631.4793855],\n",
       "       ['2017-01-31', 2629.386708333333, 1421.7832626666666,\n",
       "        553.3237412999999, 463.41115033333335, 105.35134376666666,\n",
       "        2394.287123333333, 669.1150419666667],\n",
       "       ['2017-02-28', 2622.7160196666664, 1396.6229623333334,\n",
       "        537.3870039999998, 464.47923146666665, 106.48666053333334,\n",
       "        2407.2554416666667, 706.7506984333335],\n",
       "       ['2017-03-31', 2616.045331, 1371.462662, 521.4502666999998,\n",
       "        465.5473126, 107.6219773, 2420.2237600000008, 744.3863549],\n",
       "       ['2017-04-30', 2582.999953333333, 1385.6997396666666,\n",
       "        514.2855105000001, 456.63566276666666, 110.51347520000002,\n",
       "        2409.258488666667, 756.4180495333335],\n",
       "       ['2017-05-31', 2549.9545756666666, 1399.9368173333332,\n",
       "        507.12075430000004, 447.72401293333337, 113.4049731,\n",
       "        2398.293217333333, 768.4497441666666],\n",
       "       ['2017-06-30', 2516.909198, 1414.173895, 499.9559981, 438.8123631,\n",
       "        116.296471, 2387.327946, 780.4814388],\n",
       "       ['2017-07-31', 2529.782464, 1411.1019353333334,\n",
       "        495.39090156666674, 419.9090564333334, 113.3286994,\n",
       "        2482.2845416666664, 773.9692328666665],\n",
       "       ['2017-08-31', 2542.65573, 1408.0299756666666, 490.8258050333334,\n",
       "        401.00574976666667, 110.3609278, 2577.241137333333,\n",
       "        767.4570269333334],\n",
       "       ['2017-09-30', 2555.528996, 1404.958016, 486.2607085, 382.1024431,\n",
       "        107.3931562, 2672.197733, 760.944821],\n",
       "       ['2017-10-31', 2493.5604476666667, 1340.4287853333333,\n",
       "        485.59986143333333, 386.4377233, 106.82599553333333,\n",
       "        2640.667416333333, 748.7807167999999],\n",
       "       ['2017-11-30', 2431.591899333333, 1275.8995546666667,\n",
       "        484.9390143666667, 390.7730035, 106.25883486666667,\n",
       "        2609.1370996666665, 736.6166125999998],\n",
       "       ['2017-12-31', 2369.623351, 1211.370324, 484.27816730000006,\n",
       "        395.1082837, 105.6916742, 2577.606783, 724.4525083999998],\n",
       "       ['2018-01-31', 2307.4944116666666, 1212.1657916666666,\n",
       "        479.0572197666667, 392.9066318, 102.59903074333334,\n",
       "        2577.6159756666666, 729.9813198666666],\n",
       "       ['2018-02-28', 2245.365472333333, 1212.9612593333334,\n",
       "        473.83627223333326, 390.7049799, 99.50638728666668,\n",
       "        2577.625168333333, 735.5101313333333],\n",
       "       ['2018-03-31', 2183.236533, 1213.756727, 468.6153247, 388.503328,\n",
       "        96.41374383, 2577.634361, 741.0389428],\n",
       "       ['2018-04-30', 2086.8807423333333, 1146.0012783333334,\n",
       "        448.50893473333326, 390.50832306666666, 98.85220068666668,\n",
       "        2608.0187796666664, 740.6812809333335],\n",
       "       ['2018-05-31', 1990.5249516666665, 1078.2458296666666,\n",
       "        428.40254476666667, 392.51331813333326, 101.29065754333334,\n",
       "        2638.403198333333, 740.3236190666668],\n",
       "       ['2018-06-30', 1894.169161, 1010.490381, 408.2961548, 394.5183132,\n",
       "        103.7291144, 2668.787617, 739.9659572],\n",
       "       ['2018-07-31', 1839.681530666667, 991.3822151, 434.8828281333333,\n",
       "        404.4480508333333, 100.92708618333334, 2732.0093463333333,\n",
       "        736.5653751333334],\n",
       "       ['2018-08-31', 1785.1939003333332, 972.2740492,\n",
       "        461.46950146666666, 414.37778846666674, 98.12505796666667,\n",
       "        2795.231075666666, 733.1647930666667],\n",
       "       ['2018-09-30', 1730.70627, 953.1658833, 488.0561748, 424.3075261,\n",
       "        95.32302975, 2858.452805, 729.764211],\n",
       "       ['2018-10-31', 1667.921494, 856.9003839999998, 471.2615347666666,\n",
       "        434.87458276666666, 92.14422016333332, 2849.606546,\n",
       "        737.7137843333335],\n",
       "       ['2018-11-30', 1605.136718, 760.6348846999998, 454.4668947333333,\n",
       "        445.44163943333336, 88.96541057666668, 2840.760287,\n",
       "        745.6633576666667],\n",
       "       ['2018-12-31', 1542.351942, 664.3693853999998, 437.67225470000005,\n",
       "        456.0086961, 85.78660099, 2831.914028, 753.612931]], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[:,1:5]\n",
    "Y = dataset[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_scale = min_max_scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.21536771, 0.34358992, 0.28760773, 1.        ],\n",
       "       [0.21549004, 0.35192007, 0.32225043, 0.9605532 ],\n",
       "       [0.21561238, 0.36025022, 0.35689312, 0.92110641],\n",
       "       [0.21573472, 0.36858036, 0.39153581, 0.88165961],\n",
       "       [0.21309775, 0.37683871, 0.30995679, 0.58777307],\n",
       "       [0.21046078, 0.38509706, 0.22837778, 0.29388654],\n",
       "       [0.2078238 , 0.3933554 , 0.14679876, 0.        ],\n",
       "       [0.19576386, 0.38652631, 0.09786584, 0.0434695 ],\n",
       "       [0.18370391, 0.37969722, 0.04893292, 0.086939  ],\n",
       "       [0.17164397, 0.37286814, 0.        , 0.1304085 ],\n",
       "       [0.19455526, 0.37922834, 0.05993416, 0.14266337],\n",
       "       [0.21746656, 0.38558854, 0.11986831, 0.15491825],\n",
       "       [0.24037786, 0.39194874, 0.17980247, 0.16717312],\n",
       "       [0.18782438, 0.37456299, 0.15952074, 0.15377192],\n",
       "       [0.1352709 , 0.35717725, 0.13923902, 0.14037072],\n",
       "       [0.08271742, 0.33979151, 0.1189573 , 0.12696952],\n",
       "       [0.09769806, 0.30789075, 0.12775618, 0.12639516],\n",
       "       [0.11267869, 0.27598999, 0.13655507, 0.1258208 ],\n",
       "       [0.12765932, 0.24408924, 0.14535396, 0.12524645],\n",
       "       [0.14712547, 0.23240637, 0.17879331, 0.12504798],\n",
       "       [0.16659163, 0.2207235 , 0.21223267, 0.12484952],\n",
       "       [0.18605779, 0.20904063, 0.24567202, 0.12465105],\n",
       "       [0.21580723, 0.2124702 , 0.26898377, 0.13899188],\n",
       "       [0.24555667, 0.21589978, 0.29229552, 0.1533327 ],\n",
       "       [0.27530612, 0.21932935, 0.31560727, 0.16767352],\n",
       "       [0.23174656, 0.21388031, 0.31771115, 0.17102246],\n",
       "       [0.18818701, 0.20843127, 0.31981503, 0.1743714 ],\n",
       "       [0.14462746, 0.20298222, 0.32191891, 0.17772034],\n",
       "       [0.16559061, 0.20759269, 0.37590033, 0.16918408],\n",
       "       [0.18655375, 0.21220316, 0.42988176, 0.16064782],\n",
       "       [0.2075169 , 0.21681363, 0.48386319, 0.15211155],\n",
       "       [0.20632937, 0.24717245, 0.42137913, 0.18968911],\n",
       "       [0.20514183, 0.27753127, 0.35889507, 0.22726666],\n",
       "       [0.2039543 , 0.30789009, 0.29641102, 0.26484421],\n",
       "       [0.20786336, 0.22819714, 0.32575861, 0.27218252],\n",
       "       [0.21177242, 0.14850418, 0.3551062 , 0.27952084],\n",
       "       [0.21568148, 0.06881122, 0.38445379, 0.28685915],\n",
       "       [0.22288331, 0.09770397, 0.42716844, 0.28832255],\n",
       "       [0.23008515, 0.12659671, 0.46988309, 0.28978595],\n",
       "       [0.23728698, 0.15548945, 0.51259773, 0.29124936],\n",
       "       [0.20492341, 0.1311615 , 0.48510399, 0.2922712 ],\n",
       "       [0.17255984, 0.10683355, 0.45761024, 0.29329304],\n",
       "       [0.14019627, 0.08250559, 0.4301165 , 0.29431489],\n",
       "       [0.1424773 , 0.10285422, 0.44797618, 0.28115588],\n",
       "       [0.14475833, 0.12320284, 0.46583586, 0.26799687],\n",
       "       [0.14703936, 0.14355146, 0.48369555, 0.25483787],\n",
       "       [0.15047162, 0.16464487, 0.48776269, 0.26641758],\n",
       "       [0.15390388, 0.18573829, 0.49182984, 0.2779973 ],\n",
       "       [0.15733614, 0.2068317 , 0.49589698, 0.28957702],\n",
       "       [0.14472048, 0.21745956, 0.47630548, 0.3036436 ],\n",
       "       [0.13210482, 0.22808742, 0.45671398, 0.31771019],\n",
       "       [0.11948917, 0.23871528, 0.43712248, 0.33177677],\n",
       "       [0.11305686, 0.2493644 , 0.4036701 , 0.31584481],\n",
       "       [0.10662455, 0.26001352, 0.37021773, 0.29991284],\n",
       "       [0.10019224, 0.27066264, 0.33676536, 0.28398088],\n",
       "       [0.11716762, 0.27534652, 0.31046496, 0.28570662],\n",
       "       [0.13414299, 0.28003041, 0.28416457, 0.28743236],\n",
       "       [0.15111837, 0.2847143 , 0.25786417, 0.2891581 ],\n",
       "       [0.11509853, 0.33179719, 0.29836928, 0.42623956],\n",
       "       [0.07907869, 0.37888008, 0.33887439, 0.56332102],\n",
       "       [0.04305886, 0.42596297, 0.3793795 , 0.70040248],\n",
       "       [0.12850161, 0.42978373, 0.40598475, 0.70070225],\n",
       "       [0.21394436, 0.43360448, 0.43259   , 0.70100203],\n",
       "       [0.2993871 , 0.43742523, 0.45919525, 0.70130181],\n",
       "       [0.35577117, 0.47608186, 0.46826847, 0.69781819],\n",
       "       [0.41215523, 0.5147385 , 0.47734168, 0.69433458],\n",
       "       [0.46853929, 0.55339513, 0.4864149 , 0.69085096],\n",
       "       [0.62096219, 0.58363857, 0.59803869, 0.65473261],\n",
       "       [0.7733851 , 0.613882  , 0.70966248, 0.61861427],\n",
       "       [0.925808  , 0.64412543, 0.82128627, 0.58249592],\n",
       "       [0.85725335, 0.66694505, 0.87774568, 0.58278676],\n",
       "       [0.78869869, 0.68976468, 0.93420509, 0.58307761],\n",
       "       [0.72014404, 0.7125843 , 0.9906645 , 0.58336845],\n",
       "       [0.81342936, 0.77357499, 0.99377633, 0.5692078 ],\n",
       "       [0.90671468, 0.83456568, 0.99688817, 0.55504714],\n",
       "       [1.        , 0.89555637, 1.        , 0.54088649],\n",
       "       [0.95619092, 0.93037091, 0.92133861, 0.52386224],\n",
       "       [0.91238183, 0.96518546, 0.84267722, 0.50683799],\n",
       "       [0.86857275, 1.        , 0.76401584, 0.48981373],\n",
       "       [0.90491222, 0.97987772, 0.77356422, 0.52393003],\n",
       "       [0.9412517 , 0.95975545, 0.78311261, 0.55804632],\n",
       "       [0.97759118, 0.93963317, 0.792661  , 0.59216262],\n",
       "       [0.88129339, 0.91384617, 0.73560682, 0.56296047],\n",
       "       [0.78499559, 0.88805917, 0.67855264, 0.53375832],\n",
       "       [0.6886978 , 0.86227217, 0.62149846, 0.50455617],\n",
       "       [0.68449732, 0.83454953, 0.56489702, 0.50634891],\n",
       "       [0.68029684, 0.80682688, 0.50829558, 0.50814165],\n",
       "       [0.67609636, 0.77910423, 0.45169414, 0.50993438],\n",
       "       [0.65528794, 0.79479123, 0.42624755, 0.49497649],\n",
       "       [0.63447952, 0.81047822, 0.40080097, 0.48001859],\n",
       "       [0.61367111, 0.82616521, 0.37535438, 0.46506069],\n",
       "       [0.6217773 , 0.8227804 , 0.35914083, 0.43333214],\n",
       "       [0.6298835 , 0.81939559, 0.34292729, 0.40160358],\n",
       "       [0.63798969, 0.81601078, 0.32671374, 0.36987503],\n",
       "       [0.59896858, 0.74490984, 0.32436665, 0.37715165],\n",
       "       [0.55994747, 0.6738089 , 0.32201957, 0.38442827],\n",
       "       [0.52092636, 0.60270796, 0.31967248, 0.39170489],\n",
       "       [0.48180425, 0.60358444, 0.30112959, 0.38800949],\n",
       "       [0.44268214, 0.60446092, 0.2825867 , 0.38431409],\n",
       "       [0.40356003, 0.6053374 , 0.26404381, 0.3806187 ],\n",
       "       [0.34288554, 0.53068168, 0.19263329, 0.38398401],\n",
       "       [0.28221105, 0.45602596, 0.12122277, 0.38734933],\n",
       "       [0.22153656, 0.38137024, 0.04981225, 0.39071464],\n",
       "       [0.18722612, 0.36031608, 0.14423836, 0.40738137],\n",
       "       [0.15291569, 0.33926193, 0.23866447, 0.42404809],\n",
       "       [0.11860525, 0.31820777, 0.33309058, 0.44071482],\n",
       "       [0.07907017, 0.21213851, 0.27344218, 0.45845126],\n",
       "       [0.03953508, 0.10606926, 0.21379378, 0.4761877 ],\n",
       "       [0.        , 0.        , 0.15414538, 0.49392414]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87, 4) (11, 4) (11, 4) (87,) (11,) (11,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_scale, Y, test_size=0.2)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)\n",
    "print(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(6, activation='elu', input_shape=(4,)),\n",
    "    Dense(32, activation='elu'),\n",
    "    Dense(12, activation='elu'),\n",
    "    Dense(1, activation='elu'),\n",
    "])\n",
    "#softplus, selu, elu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 87 samples, validate on 11 samples\n",
      "Epoch 1/1500\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 182.4148 - val_loss: 187.8677\n",
      "Epoch 2/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 182.3815 - val_loss: 187.8362\n",
      "Epoch 3/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 182.3471 - val_loss: 187.8026\n",
      "Epoch 4/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 182.3112 - val_loss: 187.7689\n",
      "Epoch 5/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 182.2743 - val_loss: 187.7347\n",
      "Epoch 6/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 182.2374 - val_loss: 187.7001\n",
      "Epoch 7/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 182.1996 - val_loss: 187.6650\n",
      "Epoch 8/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 182.1605 - val_loss: 187.6293\n",
      "Epoch 9/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 182.1213 - val_loss: 187.5925\n",
      "Epoch 10/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 182.0801 - val_loss: 187.5550\n",
      "Epoch 11/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 182.0385 - val_loss: 187.5159\n",
      "Epoch 12/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 181.9948 - val_loss: 187.4755\n",
      "Epoch 13/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 181.9504 - val_loss: 187.4335\n",
      "Epoch 14/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 181.9032 - val_loss: 187.3896\n",
      "Epoch 15/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 181.8542 - val_loss: 187.3435\n",
      "Epoch 16/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 181.8036 - val_loss: 187.2954\n",
      "Epoch 17/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 181.7499 - val_loss: 187.2450\n",
      "Epoch 18/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 181.6948 - val_loss: 187.1925\n",
      "Epoch 19/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 181.6368 - val_loss: 187.1372\n",
      "Epoch 20/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 181.5759 - val_loss: 187.0793\n",
      "Epoch 21/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 181.5113 - val_loss: 187.0184\n",
      "Epoch 22/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 181.4445 - val_loss: 186.9543\n",
      "Epoch 23/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 181.3748 - val_loss: 186.8869\n",
      "Epoch 24/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 181.3012 - val_loss: 186.8165\n",
      "Epoch 25/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 181.2236 - val_loss: 186.7423\n",
      "Epoch 26/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 181.1426 - val_loss: 186.6642\n",
      "Epoch 27/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 181.0573 - val_loss: 186.5824\n",
      "Epoch 28/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 180.9675 - val_loss: 186.4960\n",
      "Epoch 29/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 180.8733 - val_loss: 186.4052\n",
      "Epoch 30/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 180.7747 - val_loss: 186.3093\n",
      "Epoch 31/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 180.6698 - val_loss: 186.2083\n",
      "Epoch 32/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 180.5600 - val_loss: 186.1018\n",
      "Epoch 33/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 180.4448 - val_loss: 185.9897\n",
      "Epoch 34/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 180.3235 - val_loss: 185.8723\n",
      "Epoch 35/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 180.1962 - val_loss: 185.7483\n",
      "Epoch 36/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 180.0623 - val_loss: 185.6180\n",
      "Epoch 37/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 179.9223 - val_loss: 185.4811\n",
      "Epoch 38/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 179.7735 - val_loss: 185.3379\n",
      "Epoch 39/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 179.6203 - val_loss: 185.1874\n",
      "Epoch 40/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 179.4587 - val_loss: 185.0291\n",
      "Epoch 41/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 179.2891 - val_loss: 184.8638\n",
      "Epoch 42/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 179.1100 - val_loss: 184.6906\n",
      "Epoch 43/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 178.9267 - val_loss: 184.5090\n",
      "Epoch 44/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 178.7305 - val_loss: 184.3188\n",
      "Epoch 45/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 178.5271 - val_loss: 184.1195\n",
      "Epoch 46/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 178.3149 - val_loss: 183.9110\n",
      "Epoch 47/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 178.0911 - val_loss: 183.6932\n",
      "Epoch 48/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 177.8577 - val_loss: 183.4653\n",
      "Epoch 49/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 177.6164 - val_loss: 183.2278\n",
      "Epoch 50/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 177.3620 - val_loss: 182.9793\n",
      "Epoch 51/1500\n",
      "87/87 [==============================] - 0s 104us/step - loss: 177.0997 - val_loss: 182.7203\n",
      "Epoch 52/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 176.8222 - val_loss: 182.4512\n",
      "Epoch 53/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 176.5348 - val_loss: 182.1700\n",
      "Epoch 54/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 176.2369 - val_loss: 181.8767\n",
      "Epoch 55/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 175.9218 - val_loss: 181.5711\n",
      "Epoch 56/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 175.6008 - val_loss: 181.2523\n",
      "Epoch 57/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 175.2602 - val_loss: 180.9210\n",
      "Epoch 58/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 174.9060 - val_loss: 180.5765\n",
      "Epoch 59/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 174.5426 - val_loss: 180.2175\n",
      "Epoch 60/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 174.1570 - val_loss: 179.8433\n",
      "Epoch 61/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 173.7607 - val_loss: 179.4538\n",
      "Epoch 62/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 173.3427 - val_loss: 179.0472\n",
      "Epoch 63/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 172.9135 - val_loss: 178.6231\n",
      "Epoch 64/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 172.4602 - val_loss: 178.1816\n",
      "Epoch 65/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 171.9949 - val_loss: 177.7227\n",
      "Epoch 66/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 171.5103 - val_loss: 177.2462\n",
      "Epoch 67/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 170.9981 - val_loss: 176.7508\n",
      "Epoch 68/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 170.4726 - val_loss: 176.2349\n",
      "Epoch 69/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 169.9268 - val_loss: 175.6991\n",
      "Epoch 70/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 169.3559 - val_loss: 175.1413\n",
      "Epoch 71/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 168.7653 - val_loss: 174.5612\n",
      "Epoch 72/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 168.1506 - val_loss: 173.9588\n",
      "Epoch 73/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 167.5091 - val_loss: 173.3330\n",
      "Epoch 74/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 166.8456 - val_loss: 172.6825\n",
      "Epoch 75/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 166.1629 - val_loss: 172.0074\n",
      "Epoch 76/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 165.4444 - val_loss: 171.3082\n",
      "Epoch 77/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 164.7010 - val_loss: 170.5827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 163.9362 - val_loss: 169.8298\n",
      "Epoch 79/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 163.1424 - val_loss: 169.0500\n",
      "Epoch 80/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 162.3195 - val_loss: 168.2431\n",
      "Epoch 81/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 161.4644 - val_loss: 167.4085\n",
      "Epoch 82/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 160.5745 - val_loss: 166.5440\n",
      "Epoch 83/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 159.6726 - val_loss: 165.6479\n",
      "Epoch 84/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 158.7328 - val_loss: 164.7234\n",
      "Epoch 85/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 157.7514 - val_loss: 163.7702\n",
      "Epoch 86/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 156.7434 - val_loss: 162.7850\n",
      "Epoch 87/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 155.7058 - val_loss: 161.7666\n",
      "Epoch 88/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 154.6302 - val_loss: 160.7144\n",
      "Epoch 89/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 153.5406 - val_loss: 159.6293\n",
      "Epoch 90/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 152.3799 - val_loss: 158.5132\n",
      "Epoch 91/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 151.2139 - val_loss: 157.3607\n",
      "Epoch 92/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 149.9959 - val_loss: 156.1730\n",
      "Epoch 93/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 148.7444 - val_loss: 154.9477\n",
      "Epoch 94/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 147.4674 - val_loss: 153.6848\n",
      "Epoch 95/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 146.1446 - val_loss: 152.3873\n",
      "Epoch 96/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 144.7787 - val_loss: 151.0545\n",
      "Epoch 97/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 143.3608 - val_loss: 149.6824\n",
      "Epoch 98/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 141.9351 - val_loss: 148.2671\n",
      "Epoch 99/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 140.4460 - val_loss: 146.8125\n",
      "Epoch 100/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 138.9148 - val_loss: 145.3161\n",
      "Epoch 101/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 137.3332 - val_loss: 143.7750\n",
      "Epoch 102/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 135.7133 - val_loss: 142.1859\n",
      "Epoch 103/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 134.0527 - val_loss: 140.5493\n",
      "Epoch 104/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 132.3352 - val_loss: 138.8670\n",
      "Epoch 105/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 130.5670 - val_loss: 137.1378\n",
      "Epoch 106/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 128.7586 - val_loss: 135.3607\n",
      "Epoch 107/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 126.9200 - val_loss: 133.5389\n",
      "Epoch 108/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 125.0093 - val_loss: 131.6772\n",
      "Epoch 109/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 123.0543 - val_loss: 129.7718\n",
      "Epoch 110/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 121.0395 - val_loss: 127.8171\n",
      "Epoch 111/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 119.0137 - val_loss: 125.8093\n",
      "Epoch 112/1500\n",
      "87/87 [==============================] - 0s 104us/step - loss: 116.9146 - val_loss: 123.7543\n",
      "Epoch 113/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 114.7596 - val_loss: 121.6504\n",
      "Epoch 114/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 112.5255 - val_loss: 119.4918\n",
      "Epoch 115/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 110.3586 - val_loss: 117.2815\n",
      "Epoch 116/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 108.0874 - val_loss: 115.0280\n",
      "Epoch 117/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 105.8483 - val_loss: 112.7297\n",
      "Epoch 118/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 103.7413 - val_loss: 110.4300\n",
      "Epoch 119/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 101.8281 - val_loss: 108.1310\n",
      "Epoch 120/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 99.9434 - val_loss: 106.2627\n",
      "Epoch 121/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 98.1210 - val_loss: 104.5250\n",
      "Epoch 122/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 96.3416 - val_loss: 102.8036\n",
      "Epoch 123/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 94.6608 - val_loss: 101.0759\n",
      "Epoch 124/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 93.1201 - val_loss: 99.3423\n",
      "Epoch 125/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 91.6247 - val_loss: 97.6412\n",
      "Epoch 126/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 90.2096 - val_loss: 96.3010\n",
      "Epoch 127/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 88.9275 - val_loss: 95.3136\n",
      "Epoch 128/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 87.8208 - val_loss: 94.5866\n",
      "Epoch 129/1500\n",
      "87/87 [==============================] - 0s 104us/step - loss: 87.1415 - val_loss: 93.9049\n",
      "Epoch 130/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 86.5391 - val_loss: 93.2856\n",
      "Epoch 131/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 86.0577 - val_loss: 92.6990\n",
      "Epoch 132/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 85.6976 - val_loss: 92.1356\n",
      "Epoch 133/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 85.3461 - val_loss: 91.6108\n",
      "Epoch 134/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 85.0014 - val_loss: 91.1146\n",
      "Epoch 135/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 84.7856 - val_loss: 90.6403\n",
      "Epoch 136/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 84.5651 - val_loss: 90.3987\n",
      "Epoch 137/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 84.3437 - val_loss: 90.2211\n",
      "Epoch 138/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 84.1832 - val_loss: 90.0541\n",
      "Epoch 139/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 84.0464 - val_loss: 89.8993\n",
      "Epoch 140/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 83.9307 - val_loss: 89.7582\n",
      "Epoch 141/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 83.8096 - val_loss: 89.6303\n",
      "Epoch 142/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 83.7233 - val_loss: 89.5144\n",
      "Epoch 143/1500\n",
      "87/87 [==============================] - ETA: 0s - loss: 83.72 - 0s 92us/step - loss: 83.5960 - val_loss: 89.4107\n",
      "Epoch 144/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 83.5086 - val_loss: 89.3049\n",
      "Epoch 145/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 83.4290 - val_loss: 89.1996\n",
      "Epoch 146/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 83.3421 - val_loss: 89.0986\n",
      "Epoch 147/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 83.2459 - val_loss: 89.0007\n",
      "Epoch 148/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 83.1779 - val_loss: 88.9038\n",
      "Epoch 149/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 83.1030 - val_loss: 88.8144\n",
      "Epoch 150/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 83.0537 - val_loss: 88.7310\n",
      "Epoch 151/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 82.9900 - val_loss: 88.6597\n",
      "Epoch 152/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 82.9295 - val_loss: 88.5943\n",
      "Epoch 153/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 82.8834 - val_loss: 88.5281\n",
      "Epoch 154/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 82.8262 - val_loss: 88.4656\n",
      "Epoch 155/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 69us/step - loss: 82.7738 - val_loss: 88.3988\n",
      "Epoch 156/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 82.7250 - val_loss: 88.3237\n",
      "Epoch 157/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 82.6686 - val_loss: 88.2479\n",
      "Epoch 158/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 82.6079 - val_loss: 88.1696\n",
      "Epoch 159/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 82.5644 - val_loss: 88.0878\n",
      "Epoch 160/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 82.4911 - val_loss: 88.0114\n",
      "Epoch 161/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 82.4437 - val_loss: 87.9297\n",
      "Epoch 162/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 82.3805 - val_loss: 87.8519\n",
      "Epoch 163/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 82.3190 - val_loss: 87.7731\n",
      "Epoch 164/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 82.2566 - val_loss: 87.6884\n",
      "Epoch 165/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 82.1955 - val_loss: 87.5932\n",
      "Epoch 166/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 82.1324 - val_loss: 87.4928\n",
      "Epoch 167/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 82.0742 - val_loss: 87.3932\n",
      "Epoch 168/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 81.9940 - val_loss: 87.2976\n",
      "Epoch 169/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 81.9263 - val_loss: 87.1946\n",
      "Epoch 170/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 81.8563 - val_loss: 87.0867\n",
      "Epoch 171/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 81.7980 - val_loss: 86.9771\n",
      "Epoch 172/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 81.7120 - val_loss: 86.8718\n",
      "Epoch 173/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 81.6442 - val_loss: 86.7618\n",
      "Epoch 174/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 81.5824 - val_loss: 86.6525\n",
      "Epoch 175/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 81.5027 - val_loss: 86.5497\n",
      "Epoch 176/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 81.4260 - val_loss: 86.4459\n",
      "Epoch 177/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 81.3709 - val_loss: 86.3397\n",
      "Epoch 178/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 81.2832 - val_loss: 86.2383\n",
      "Epoch 179/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 81.2332 - val_loss: 86.1334\n",
      "Epoch 180/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 81.1628 - val_loss: 86.0424\n",
      "Epoch 181/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 81.0831 - val_loss: 85.9618\n",
      "Epoch 182/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 81.0195 - val_loss: 85.8764\n",
      "Epoch 183/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 80.9721 - val_loss: 85.7830\n",
      "Epoch 184/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 80.8993 - val_loss: 85.7013\n",
      "Epoch 185/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 80.8282 - val_loss: 85.6220\n",
      "Epoch 186/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 80.7706 - val_loss: 85.5359\n",
      "Epoch 187/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 80.7187 - val_loss: 85.4521\n",
      "Epoch 188/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 80.6413 - val_loss: 85.3742\n",
      "Epoch 189/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 80.5810 - val_loss: 85.2823\n",
      "Epoch 190/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 80.5270 - val_loss: 85.1854\n",
      "Epoch 191/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 80.4465 - val_loss: 85.0942\n",
      "Epoch 192/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 80.3876 - val_loss: 84.9990\n",
      "Epoch 193/1500\n",
      "87/87 [==============================] - 0s 104us/step - loss: 80.3037 - val_loss: 84.9022\n",
      "Epoch 194/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 80.2239 - val_loss: 84.7813\n",
      "Epoch 195/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 80.1470 - val_loss: 84.6406\n",
      "Epoch 196/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 80.0909 - val_loss: 84.4996\n",
      "Epoch 197/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 79.9825 - val_loss: 84.3731\n",
      "Epoch 198/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 79.8885 - val_loss: 84.2467\n",
      "Epoch 199/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 79.8097 - val_loss: 84.1144\n",
      "Epoch 200/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 79.7291 - val_loss: 83.9828\n",
      "Epoch 201/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 79.6370 - val_loss: 83.8543\n",
      "Epoch 202/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 79.5542 - val_loss: 83.7268\n",
      "Epoch 203/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 79.4707 - val_loss: 83.5918\n",
      "Epoch 204/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 79.4019 - val_loss: 83.4438\n",
      "Epoch 205/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 79.3405 - val_loss: 83.2987\n",
      "Epoch 206/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 79.2492 - val_loss: 83.1633\n",
      "Epoch 207/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 79.1679 - val_loss: 83.0275\n",
      "Epoch 208/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 79.1101 - val_loss: 82.8906\n",
      "Epoch 209/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 79.0212 - val_loss: 82.7624\n",
      "Epoch 210/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 78.9423 - val_loss: 82.6314\n",
      "Epoch 211/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 78.8614 - val_loss: 82.4924\n",
      "Epoch 212/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 78.8220 - val_loss: 82.3528\n",
      "Epoch 213/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 78.7537 - val_loss: 82.2410\n",
      "Epoch 214/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 78.6589 - val_loss: 82.1555\n",
      "Epoch 215/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 78.5939 - val_loss: 82.0739\n",
      "Epoch 216/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 78.5317 - val_loss: 81.9922\n",
      "Epoch 217/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 78.4720 - val_loss: 81.9062\n",
      "Epoch 218/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 78.4182 - val_loss: 81.8205\n",
      "Epoch 219/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 78.3515 - val_loss: 81.7428\n",
      "Epoch 220/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 78.2989 - val_loss: 81.6688\n",
      "Epoch 221/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 78.2329 - val_loss: 81.6039\n",
      "Epoch 222/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 78.1761 - val_loss: 81.5349\n",
      "Epoch 223/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 78.1212 - val_loss: 81.4544\n",
      "Epoch 224/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 78.0659 - val_loss: 81.3713\n",
      "Epoch 225/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 77.9993 - val_loss: 81.2920\n",
      "Epoch 226/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 77.9457 - val_loss: 81.2041\n",
      "Epoch 227/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 77.8737 - val_loss: 81.1163\n",
      "Epoch 228/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 77.8175 - val_loss: 81.0144\n",
      "Epoch 229/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 77.7493 - val_loss: 80.9179\n",
      "Epoch 230/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 77.6819 - val_loss: 80.8257\n",
      "Epoch 231/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 77.6161 - val_loss: 80.7374\n",
      "Epoch 232/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 77.5553 - val_loss: 80.6534\n",
      "Epoch 233/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 92us/step - loss: 77.4902 - val_loss: 80.5759\n",
      "Epoch 234/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 77.4265 - val_loss: 80.4948\n",
      "Epoch 235/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 77.3576 - val_loss: 80.4002\n",
      "Epoch 236/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 77.2875 - val_loss: 80.2813\n",
      "Epoch 237/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 77.2276 - val_loss: 80.1470\n",
      "Epoch 238/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 77.1312 - val_loss: 80.0181\n",
      "Epoch 239/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 77.0674 - val_loss: 79.8842\n",
      "Epoch 240/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 76.9782 - val_loss: 79.7582\n",
      "Epoch 241/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 76.8828 - val_loss: 79.6229\n",
      "Epoch 242/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 76.8009 - val_loss: 79.4593\n",
      "Epoch 243/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 76.7338 - val_loss: 79.2938\n",
      "Epoch 244/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 76.6354 - val_loss: 79.1428\n",
      "Epoch 245/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 76.5633 - val_loss: 79.0049\n",
      "Epoch 246/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 76.4585 - val_loss: 78.8849\n",
      "Epoch 247/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 76.3760 - val_loss: 78.7607\n",
      "Epoch 248/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 76.3066 - val_loss: 78.6301\n",
      "Epoch 249/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 76.2204 - val_loss: 78.5020\n",
      "Epoch 250/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 76.1596 - val_loss: 78.3803\n",
      "Epoch 251/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 76.0895 - val_loss: 78.2740\n",
      "Epoch 252/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 76.0152 - val_loss: 78.1709\n",
      "Epoch 253/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 75.9444 - val_loss: 78.0502\n",
      "Epoch 254/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 75.8906 - val_loss: 77.9135\n",
      "Epoch 255/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 75.8185 - val_loss: 77.7874\n",
      "Epoch 256/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 75.7346 - val_loss: 77.6625\n",
      "Epoch 257/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 75.7012 - val_loss: 77.5317\n",
      "Epoch 258/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 75.6182 - val_loss: 77.4262\n",
      "Epoch 259/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 75.5720 - val_loss: 77.3274\n",
      "Epoch 260/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 75.5082 - val_loss: 77.2488\n",
      "Epoch 261/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 75.4502 - val_loss: 77.1769\n",
      "Epoch 262/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 75.3953 - val_loss: 77.0916\n",
      "Epoch 263/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 75.3385 - val_loss: 76.9925\n",
      "Epoch 264/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 75.2768 - val_loss: 76.8759\n",
      "Epoch 265/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 75.2069 - val_loss: 76.7385\n",
      "Epoch 266/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 75.1472 - val_loss: 76.5747\n",
      "Epoch 267/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 75.0927 - val_loss: 76.4085\n",
      "Epoch 268/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 75.0700 - val_loss: 76.2501\n",
      "Epoch 269/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 75.0032 - val_loss: 76.1185\n",
      "Epoch 270/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 74.9615 - val_loss: 76.0020\n",
      "Epoch 271/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 74.9014 - val_loss: 75.9071\n",
      "Epoch 272/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 74.8531 - val_loss: 75.8612\n",
      "Epoch 273/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 74.7971 - val_loss: 75.8180\n",
      "Epoch 274/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 74.7420 - val_loss: 75.7745\n",
      "Epoch 275/1500\n",
      "87/87 [==============================] - ETA: 0s - loss: 74.94 - 0s 80us/step - loss: 74.6924 - val_loss: 75.7300\n",
      "Epoch 276/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 74.6410 - val_loss: 75.6848\n",
      "Epoch 277/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 74.5886 - val_loss: 75.6388\n",
      "Epoch 278/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 74.5482 - val_loss: 75.5927\n",
      "Epoch 279/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 74.4848 - val_loss: 75.5477\n",
      "Epoch 280/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 74.4392 - val_loss: 75.5023\n",
      "Epoch 281/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 74.3806 - val_loss: 75.4568\n",
      "Epoch 282/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 74.3400 - val_loss: 75.4112\n",
      "Epoch 283/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 74.2822 - val_loss: 75.3673\n",
      "Epoch 284/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 74.2510 - val_loss: 75.3240\n",
      "Epoch 285/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 74.1987 - val_loss: 75.2823\n",
      "Epoch 286/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 74.1532 - val_loss: 75.2407\n",
      "Epoch 287/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 74.1094 - val_loss: 75.1997\n",
      "Epoch 288/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 74.0623 - val_loss: 75.1597\n",
      "Epoch 289/1500\n",
      "87/87 [==============================] - ETA: 0s - loss: 79.62 - 0s 81us/step - loss: 74.0156 - val_loss: 75.1200\n",
      "Epoch 290/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 73.9694 - val_loss: 75.0781\n",
      "Epoch 291/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 73.9237 - val_loss: 75.0360\n",
      "Epoch 292/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 73.8862 - val_loss: 74.9949\n",
      "Epoch 293/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 73.8289 - val_loss: 74.9557\n",
      "Epoch 294/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 73.7822 - val_loss: 74.9163\n",
      "Epoch 295/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 73.7320 - val_loss: 74.8765\n",
      "Epoch 296/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 73.6704 - val_loss: 74.8389\n",
      "Epoch 297/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 73.6271 - val_loss: 74.8028\n",
      "Epoch 298/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 73.5657 - val_loss: 74.7663\n",
      "Epoch 299/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 73.5187 - val_loss: 74.7309\n",
      "Epoch 300/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 73.4853 - val_loss: 74.6960\n",
      "Epoch 301/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 73.4430 - val_loss: 74.6611\n",
      "Epoch 302/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 73.4120 - val_loss: 74.6259\n",
      "Epoch 303/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 73.3785 - val_loss: 74.5899\n",
      "Epoch 304/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 73.3255 - val_loss: 74.5525\n",
      "Epoch 305/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 73.2806 - val_loss: 74.5155\n",
      "Epoch 306/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 73.2400 - val_loss: 74.4786\n",
      "Epoch 307/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 73.1968 - val_loss: 74.4398\n",
      "Epoch 308/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 73.1519 - val_loss: 74.3994\n",
      "Epoch 309/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 73.1122 - val_loss: 74.3591\n",
      "Epoch 310/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 92us/step - loss: 73.0602 - val_loss: 74.3163\n",
      "Epoch 311/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 73.0102 - val_loss: 74.2737\n",
      "Epoch 312/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 72.9589 - val_loss: 74.2316\n",
      "Epoch 313/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 72.9144 - val_loss: 74.1895\n",
      "Epoch 314/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 72.8639 - val_loss: 74.1489\n",
      "Epoch 315/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 72.8258 - val_loss: 74.1091\n",
      "Epoch 316/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 72.7706 - val_loss: 74.0676\n",
      "Epoch 317/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 72.7226 - val_loss: 74.0264\n",
      "Epoch 318/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 72.6740 - val_loss: 73.9858\n",
      "Epoch 319/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 72.6283 - val_loss: 73.9451\n",
      "Epoch 320/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 72.5781 - val_loss: 73.9048\n",
      "Epoch 321/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 72.5298 - val_loss: 73.8643\n",
      "Epoch 322/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 72.4825 - val_loss: 73.8225\n",
      "Epoch 323/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 72.4357 - val_loss: 73.7809\n",
      "Epoch 324/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 72.3888 - val_loss: 73.7391\n",
      "Epoch 325/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 72.3290 - val_loss: 73.6935\n",
      "Epoch 326/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 72.2866 - val_loss: 73.6470\n",
      "Epoch 327/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 72.2339 - val_loss: 73.6022\n",
      "Epoch 328/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 72.1855 - val_loss: 73.5566\n",
      "Epoch 329/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 72.1421 - val_loss: 73.5095\n",
      "Epoch 330/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 72.1069 - val_loss: 73.4628\n",
      "Epoch 331/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 72.0486 - val_loss: 73.4180\n",
      "Epoch 332/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 72.0031 - val_loss: 73.3729\n",
      "Epoch 333/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 71.9591 - val_loss: 73.3294\n",
      "Epoch 334/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 71.9072 - val_loss: 73.2881\n",
      "Epoch 335/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 71.8652 - val_loss: 73.2455\n",
      "Epoch 336/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 71.8111 - val_loss: 73.2010\n",
      "Epoch 337/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 71.7689 - val_loss: 73.1567\n",
      "Epoch 338/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 71.7240 - val_loss: 73.1141\n",
      "Epoch 339/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 71.6724 - val_loss: 73.0724\n",
      "Epoch 340/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 71.6217 - val_loss: 73.0269\n",
      "Epoch 341/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 71.5864 - val_loss: 72.9804\n",
      "Epoch 342/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 71.5212 - val_loss: 72.9370\n",
      "Epoch 343/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 71.4815 - val_loss: 72.8932\n",
      "Epoch 344/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 71.4262 - val_loss: 72.8529\n",
      "Epoch 345/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 71.3727 - val_loss: 72.8136\n",
      "Epoch 346/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 71.3247 - val_loss: 72.7752\n",
      "Epoch 347/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 71.2800 - val_loss: 72.7389\n",
      "Epoch 348/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 71.2405 - val_loss: 72.7006\n",
      "Epoch 349/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 71.1845 - val_loss: 72.6584\n",
      "Epoch 350/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 71.1387 - val_loss: 72.6144\n",
      "Epoch 351/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 71.0906 - val_loss: 72.5689\n",
      "Epoch 352/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 71.0364 - val_loss: 72.5263\n",
      "Epoch 353/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 70.9896 - val_loss: 72.4834\n",
      "Epoch 354/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 70.9407 - val_loss: 72.4378\n",
      "Epoch 355/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 70.8867 - val_loss: 72.3933\n",
      "Epoch 356/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 70.8454 - val_loss: 72.3502\n",
      "Epoch 357/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 70.7873 - val_loss: 72.3125\n",
      "Epoch 358/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 70.7512 - val_loss: 72.2731\n",
      "Epoch 359/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 70.6935 - val_loss: 72.2290\n",
      "Epoch 360/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 70.6401 - val_loss: 72.1857\n",
      "Epoch 361/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 70.5904 - val_loss: 72.1438\n",
      "Epoch 362/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 70.5435 - val_loss: 72.1015\n",
      "Epoch 363/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 70.4913 - val_loss: 72.0574\n",
      "Epoch 364/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 70.4496 - val_loss: 72.0134\n",
      "Epoch 365/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 70.3951 - val_loss: 71.9688\n",
      "Epoch 366/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 70.3458 - val_loss: 71.9266\n",
      "Epoch 367/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 70.3005 - val_loss: 71.8847\n",
      "Epoch 368/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 70.2536 - val_loss: 71.8398\n",
      "Epoch 369/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 70.2044 - val_loss: 71.7929\n",
      "Epoch 370/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 70.1563 - val_loss: 71.7478\n",
      "Epoch 371/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 70.1070 - val_loss: 71.7007\n",
      "Epoch 372/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 70.0677 - val_loss: 71.6530\n",
      "Epoch 373/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 70.0176 - val_loss: 71.6082\n",
      "Epoch 374/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 69.9739 - val_loss: 71.5646\n",
      "Epoch 375/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 69.9262 - val_loss: 71.5198\n",
      "Epoch 376/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 69.8804 - val_loss: 71.4739\n",
      "Epoch 377/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 69.8336 - val_loss: 71.4300\n",
      "Epoch 378/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 69.7824 - val_loss: 71.3860\n",
      "Epoch 379/1500\n",
      "87/87 [==============================] - 0s 218us/step - loss: 69.7422 - val_loss: 71.3415\n",
      "Epoch 380/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 69.6873 - val_loss: 71.2999\n",
      "Epoch 381/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 69.6418 - val_loss: 71.2588\n",
      "Epoch 382/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 69.5918 - val_loss: 71.2180\n",
      "Epoch 383/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 69.5444 - val_loss: 71.1763\n",
      "Epoch 384/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 69.4899 - val_loss: 71.1395\n",
      "Epoch 385/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 69.4378 - val_loss: 71.0998\n",
      "Epoch 386/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 69.3895 - val_loss: 71.0575\n",
      "Epoch 387/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 69.3347 - val_loss: 71.0143\n",
      "Epoch 388/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 92us/step - loss: 69.2886 - val_loss: 70.9719\n",
      "Epoch 389/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 69.2346 - val_loss: 70.9286\n",
      "Epoch 390/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 69.1841 - val_loss: 70.8875\n",
      "Epoch 391/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 69.1338 - val_loss: 70.8506\n",
      "Epoch 392/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 69.0835 - val_loss: 70.8158\n",
      "Epoch 393/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 69.0407 - val_loss: 70.7784\n",
      "Epoch 394/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 68.9798 - val_loss: 70.7353\n",
      "Epoch 395/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 68.9364 - val_loss: 70.6907\n",
      "Epoch 396/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 68.8843 - val_loss: 70.6513\n",
      "Epoch 397/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 68.8263 - val_loss: 70.6117\n",
      "Epoch 398/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 68.7736 - val_loss: 70.5711\n",
      "Epoch 399/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 68.7226 - val_loss: 70.5310\n",
      "Epoch 400/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 68.6750 - val_loss: 70.4905\n",
      "Epoch 401/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 68.6164 - val_loss: 70.4490\n",
      "Epoch 402/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 68.5588 - val_loss: 70.4110\n",
      "Epoch 403/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 68.5241 - val_loss: 70.3753\n",
      "Epoch 404/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 68.4521 - val_loss: 70.3353\n",
      "Epoch 405/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 68.4115 - val_loss: 70.2988\n",
      "Epoch 406/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 68.3467 - val_loss: 70.2585\n",
      "Epoch 407/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 68.2971 - val_loss: 70.2185\n",
      "Epoch 408/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 68.2440 - val_loss: 70.1756\n",
      "Epoch 409/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 68.1899 - val_loss: 70.1298\n",
      "Epoch 410/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 68.1413 - val_loss: 70.0804\n",
      "Epoch 411/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 68.0936 - val_loss: 70.0271\n",
      "Epoch 412/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 68.0397 - val_loss: 69.9797\n",
      "Epoch 413/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 67.9850 - val_loss: 69.9355\n",
      "Epoch 414/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 67.9309 - val_loss: 69.8862\n",
      "Epoch 415/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 67.8790 - val_loss: 69.8344\n",
      "Epoch 416/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 67.8247 - val_loss: 69.7831\n",
      "Epoch 417/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 67.7721 - val_loss: 69.7301\n",
      "Epoch 418/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 67.7195 - val_loss: 69.6799\n",
      "Epoch 419/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 67.6584 - val_loss: 69.6292\n",
      "Epoch 420/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 67.6047 - val_loss: 69.5786\n",
      "Epoch 421/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 67.5462 - val_loss: 69.5243\n",
      "Epoch 422/1500\n",
      "87/87 [==============================] - ETA: 0s - loss: 73.45 - 0s 80us/step - loss: 67.4892 - val_loss: 69.4696\n",
      "Epoch 423/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 67.4317 - val_loss: 69.4159\n",
      "Epoch 424/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 67.3718 - val_loss: 69.3614\n",
      "Epoch 425/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 67.3152 - val_loss: 69.3031\n",
      "Epoch 426/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 67.2522 - val_loss: 69.2393\n",
      "Epoch 427/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 67.1955 - val_loss: 69.1766\n",
      "Epoch 428/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 67.1287 - val_loss: 69.1159\n",
      "Epoch 429/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 67.0733 - val_loss: 69.0552\n",
      "Epoch 430/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 67.0195 - val_loss: 68.9936\n",
      "Epoch 431/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 66.9691 - val_loss: 68.9303\n",
      "Epoch 432/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 66.9184 - val_loss: 68.8713\n",
      "Epoch 433/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 66.8617 - val_loss: 68.8175\n",
      "Epoch 434/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 66.8047 - val_loss: 68.7662\n",
      "Epoch 435/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 66.7431 - val_loss: 68.7176\n",
      "Epoch 436/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 66.6869 - val_loss: 68.6743\n",
      "Epoch 437/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 66.6325 - val_loss: 68.6261\n",
      "Epoch 438/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 66.5515 - val_loss: 68.5704\n",
      "Epoch 439/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 66.4974 - val_loss: 68.5142\n",
      "Epoch 440/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 66.4275 - val_loss: 68.4626\n",
      "Epoch 441/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 66.3655 - val_loss: 68.4123\n",
      "Epoch 442/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 66.2967 - val_loss: 68.3630\n",
      "Epoch 443/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 66.2221 - val_loss: 68.3135\n",
      "Epoch 444/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 66.1726 - val_loss: 68.2638\n",
      "Epoch 445/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 66.0821 - val_loss: 68.2086\n",
      "Epoch 446/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 66.0184 - val_loss: 68.1554\n",
      "Epoch 447/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 65.9386 - val_loss: 68.0980\n",
      "Epoch 448/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 65.8778 - val_loss: 68.0381\n",
      "Epoch 449/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 65.8024 - val_loss: 67.9729\n",
      "Epoch 450/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 65.7379 - val_loss: 67.9105\n",
      "Epoch 451/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 65.6629 - val_loss: 67.8427\n",
      "Epoch 452/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 65.5999 - val_loss: 67.7811\n",
      "Epoch 453/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 65.5220 - val_loss: 67.7268\n",
      "Epoch 454/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 65.4624 - val_loss: 67.6742\n",
      "Epoch 455/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 65.3825 - val_loss: 67.6184\n",
      "Epoch 456/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 65.3118 - val_loss: 67.5611\n",
      "Epoch 457/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 65.2319 - val_loss: 67.5106\n",
      "Epoch 458/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 65.1589 - val_loss: 67.4643\n",
      "Epoch 459/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 65.0947 - val_loss: 67.4135\n",
      "Epoch 460/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 65.0039 - val_loss: 67.3568\n",
      "Epoch 461/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 64.9294 - val_loss: 67.2975\n",
      "Epoch 462/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 64.8543 - val_loss: 67.2342\n",
      "Epoch 463/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 64.7728 - val_loss: 67.1781\n",
      "Epoch 464/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 64.6925 - val_loss: 67.1189\n",
      "Epoch 465/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 64.6148 - val_loss: 67.0611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 466/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 64.5340 - val_loss: 67.0023\n",
      "Epoch 467/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 64.4554 - val_loss: 66.9444\n",
      "Epoch 468/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 64.3670 - val_loss: 66.8910\n",
      "Epoch 469/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 64.2876 - val_loss: 66.8333\n",
      "Epoch 470/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 64.2138 - val_loss: 66.7732\n",
      "Epoch 471/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 64.1165 - val_loss: 66.7053\n",
      "Epoch 472/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 64.0173 - val_loss: 66.6377\n",
      "Epoch 473/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 63.9400 - val_loss: 66.5698\n",
      "Epoch 474/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 63.8421 - val_loss: 66.5013\n",
      "Epoch 475/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 63.7457 - val_loss: 66.4303\n",
      "Epoch 476/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 63.6528 - val_loss: 66.3573\n",
      "Epoch 477/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 63.5550 - val_loss: 66.2849\n",
      "Epoch 478/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 63.4550 - val_loss: 66.2126\n",
      "Epoch 479/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 63.3501 - val_loss: 66.1393\n",
      "Epoch 480/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 63.2584 - val_loss: 66.0655\n",
      "Epoch 481/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 63.1428 - val_loss: 65.9878\n",
      "Epoch 482/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 63.0416 - val_loss: 65.9077\n",
      "Epoch 483/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 62.9315 - val_loss: 65.8296\n",
      "Epoch 484/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 62.8311 - val_loss: 65.7481\n",
      "Epoch 485/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 62.7174 - val_loss: 65.6707\n",
      "Epoch 486/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 62.5916 - val_loss: 65.5971\n",
      "Epoch 487/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 62.4899 - val_loss: 65.5188\n",
      "Epoch 488/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 62.3595 - val_loss: 65.4384\n",
      "Epoch 489/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 62.2342 - val_loss: 65.3602\n",
      "Epoch 490/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 62.1182 - val_loss: 65.2851\n",
      "Epoch 491/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 61.9784 - val_loss: 65.2097\n",
      "Epoch 492/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 61.8498 - val_loss: 65.1378\n",
      "Epoch 493/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 61.7200 - val_loss: 65.0723\n",
      "Epoch 494/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 61.5956 - val_loss: 64.9971\n",
      "Epoch 495/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 61.4597 - val_loss: 64.9089\n",
      "Epoch 496/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 61.3202 - val_loss: 64.8154\n",
      "Epoch 497/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 61.1746 - val_loss: 64.7162\n",
      "Epoch 498/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 61.0258 - val_loss: 64.6115\n",
      "Epoch 499/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 60.8668 - val_loss: 64.5001\n",
      "Epoch 500/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 60.7179 - val_loss: 64.3831\n",
      "Epoch 501/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 60.5521 - val_loss: 64.2650\n",
      "Epoch 502/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 60.3934 - val_loss: 64.1506\n",
      "Epoch 503/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 60.2116 - val_loss: 64.0375\n",
      "Epoch 504/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 60.0565 - val_loss: 63.9269\n",
      "Epoch 505/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 59.8718 - val_loss: 63.8102\n",
      "Epoch 506/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 59.6842 - val_loss: 63.6793\n",
      "Epoch 507/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 59.4982 - val_loss: 63.5438\n",
      "Epoch 508/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 59.3235 - val_loss: 63.4053\n",
      "Epoch 509/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 59.1262 - val_loss: 63.2622\n",
      "Epoch 510/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 58.9254 - val_loss: 63.1185\n",
      "Epoch 511/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 58.7204 - val_loss: 62.9793\n",
      "Epoch 512/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 58.5225 - val_loss: 62.8400\n",
      "Epoch 513/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 58.3203 - val_loss: 62.6937\n",
      "Epoch 514/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 58.0930 - val_loss: 62.5494\n",
      "Epoch 515/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 57.8777 - val_loss: 62.4055\n",
      "Epoch 516/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 57.6531 - val_loss: 62.2643\n",
      "Epoch 517/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 57.4371 - val_loss: 62.1276\n",
      "Epoch 518/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 57.2146 - val_loss: 61.9837\n",
      "Epoch 519/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 57.0087 - val_loss: 61.9759\n",
      "Epoch 520/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 56.8624 - val_loss: 61.9819\n",
      "Epoch 521/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 56.6929 - val_loss: 61.9303\n",
      "Epoch 522/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 56.5075 - val_loss: 61.8548\n",
      "Epoch 523/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 56.3239 - val_loss: 61.7871\n",
      "Epoch 524/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 56.1556 - val_loss: 61.7176\n",
      "Epoch 525/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 55.9848 - val_loss: 61.6001\n",
      "Epoch 526/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 55.8009 - val_loss: 61.4669\n",
      "Epoch 527/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 55.6094 - val_loss: 61.3044\n",
      "Epoch 528/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 55.4229 - val_loss: 61.1104\n",
      "Epoch 529/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 55.2430 - val_loss: 60.9200\n",
      "Epoch 530/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 55.0413 - val_loss: 60.7330\n",
      "Epoch 531/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 54.8525 - val_loss: 60.5683\n",
      "Epoch 532/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 54.6508 - val_loss: 60.4260\n",
      "Epoch 533/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 54.4723 - val_loss: 60.2412\n",
      "Epoch 534/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 54.3176 - val_loss: 60.0690\n",
      "Epoch 535/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 54.1108 - val_loss: 59.9056\n",
      "Epoch 536/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 53.9310 - val_loss: 59.7366\n",
      "Epoch 537/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 53.7528 - val_loss: 59.5646\n",
      "Epoch 538/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 53.5915 - val_loss: 59.3768\n",
      "Epoch 539/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 53.4013 - val_loss: 59.2411\n",
      "Epoch 540/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 53.2265 - val_loss: 59.1240\n",
      "Epoch 541/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 53.0934 - val_loss: 59.0163\n",
      "Epoch 542/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 52.9496 - val_loss: 58.9229\n",
      "Epoch 543/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 52.8630 - val_loss: 58.8423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 544/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 52.7258 - val_loss: 58.8444\n",
      "Epoch 545/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 52.6121 - val_loss: 58.8919\n",
      "Epoch 546/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 52.5113 - val_loss: 58.9022\n",
      "Epoch 547/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 52.4940 - val_loss: 58.8658\n",
      "Epoch 548/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 52.4117 - val_loss: 58.7833\n",
      "Epoch 549/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 52.3801 - val_loss: 58.7385\n",
      "Epoch 550/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 52.3456 - val_loss: 58.7470\n",
      "Epoch 551/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 52.3431 - val_loss: 58.7841\n",
      "Epoch 552/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 52.3055 - val_loss: 58.7776\n",
      "Epoch 553/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 52.3021 - val_loss: 58.7182\n",
      "Epoch 554/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 52.3102 - val_loss: 58.6244\n",
      "Epoch 555/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 52.2609 - val_loss: 58.5689\n",
      "Epoch 556/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 52.2344 - val_loss: 58.4633\n",
      "Epoch 557/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 52.2173 - val_loss: 58.3249\n",
      "Epoch 558/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 52.2255 - val_loss: 58.2106\n",
      "Epoch 559/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 52.2096 - val_loss: 58.1404\n",
      "Epoch 560/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 52.1968 - val_loss: 58.0948\n",
      "Epoch 561/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 52.1804 - val_loss: 58.0763\n",
      "Epoch 562/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 52.1669 - val_loss: 58.0731\n",
      "Epoch 563/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 52.1538 - val_loss: 58.0792\n",
      "Epoch 564/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 52.1238 - val_loss: 58.1507\n",
      "Epoch 565/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 52.0883 - val_loss: 58.1871\n",
      "Epoch 566/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 52.0634 - val_loss: 58.2178\n",
      "Epoch 567/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 52.0558 - val_loss: 58.2560\n",
      "Epoch 568/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 52.0458 - val_loss: 58.2465\n",
      "Epoch 569/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 52.0404 - val_loss: 58.1828\n",
      "Epoch 570/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 52.0083 - val_loss: 58.0577\n",
      "Epoch 571/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.9985 - val_loss: 57.9466\n",
      "Epoch 572/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 51.9685 - val_loss: 57.8716\n",
      "Epoch 573/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 51.9496 - val_loss: 57.7732\n",
      "Epoch 574/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.9395 - val_loss: 57.6763\n",
      "Epoch 575/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.9302 - val_loss: 57.5805\n",
      "Epoch 576/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.9309 - val_loss: 57.4986\n",
      "Epoch 577/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.9189 - val_loss: 57.4636\n",
      "Epoch 578/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.9160 - val_loss: 57.4363\n",
      "Epoch 579/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.8793 - val_loss: 57.3575\n",
      "Epoch 580/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.8736 - val_loss: 57.3223\n",
      "Epoch 581/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 51.8610 - val_loss: 57.3294\n",
      "Epoch 582/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.8312 - val_loss: 57.3214\n",
      "Epoch 583/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 51.8127 - val_loss: 57.3320\n",
      "Epoch 584/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 51.7957 - val_loss: 57.3219\n",
      "Epoch 585/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.7706 - val_loss: 57.2958\n",
      "Epoch 586/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 51.7532 - val_loss: 57.3080\n",
      "Epoch 587/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.7334 - val_loss: 57.3704\n",
      "Epoch 588/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 51.7235 - val_loss: 57.3839\n",
      "Epoch 589/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.7109 - val_loss: 57.3473\n",
      "Epoch 590/1500\n",
      "87/87 [==============================] - ETA: 0s - loss: 33.18 - 0s 81us/step - loss: 51.6925 - val_loss: 57.2606\n",
      "Epoch 591/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.6779 - val_loss: 57.1610\n",
      "Epoch 592/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 51.6552 - val_loss: 57.0851\n",
      "Epoch 593/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 51.6436 - val_loss: 57.0311\n",
      "Epoch 594/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 51.6341 - val_loss: 57.0178\n",
      "Epoch 595/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.5994 - val_loss: 57.0617\n",
      "Epoch 596/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.5881 - val_loss: 57.1064\n",
      "Epoch 597/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.6016 - val_loss: 57.1244\n",
      "Epoch 598/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.5740 - val_loss: 57.0598\n",
      "Epoch 599/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 51.5704 - val_loss: 56.9821\n",
      "Epoch 600/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.5374 - val_loss: 56.9341\n",
      "Epoch 601/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.5096 - val_loss: 56.8437\n",
      "Epoch 602/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.5146 - val_loss: 56.7518\n",
      "Epoch 603/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 51.5050 - val_loss: 56.6807\n",
      "Epoch 604/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 51.5022 - val_loss: 56.6141\n",
      "Epoch 605/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.4939 - val_loss: 56.5816\n",
      "Epoch 606/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.4778 - val_loss: 56.5847\n",
      "Epoch 607/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.4652 - val_loss: 56.6251\n",
      "Epoch 608/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.4316 - val_loss: 56.6490\n",
      "Epoch 609/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.4189 - val_loss: 56.6642\n",
      "Epoch 610/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.4058 - val_loss: 56.6468\n",
      "Epoch 611/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 51.3896 - val_loss: 56.6002\n",
      "Epoch 612/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.3830 - val_loss: 56.5584\n",
      "Epoch 613/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.3669 - val_loss: 56.5438\n",
      "Epoch 614/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 51.3491 - val_loss: 56.5631\n",
      "Epoch 615/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.3442 - val_loss: 56.5722\n",
      "Epoch 616/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.3397 - val_loss: 56.5418\n",
      "Epoch 617/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.3270 - val_loss: 56.5021\n",
      "Epoch 618/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.3112 - val_loss: 56.4407\n",
      "Epoch 619/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.2957 - val_loss: 56.3549\n",
      "Epoch 620/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 51.2754 - val_loss: 56.2882\n",
      "Epoch 621/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 51.2610 - val_loss: 56.1939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 622/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 51.2472 - val_loss: 56.0807\n",
      "Epoch 623/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 51.2411 - val_loss: 55.9662\n",
      "Epoch 624/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.2509 - val_loss: 55.8655\n",
      "Epoch 625/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.2458 - val_loss: 55.8375\n",
      "Epoch 626/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.2256 - val_loss: 55.8622\n",
      "Epoch 627/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 51.2074 - val_loss: 55.8951\n",
      "Epoch 628/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 51.1836 - val_loss: 55.9367\n",
      "Epoch 629/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.1708 - val_loss: 55.9819\n",
      "Epoch 630/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 51.1794 - val_loss: 55.9903\n",
      "Epoch 631/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 51.1513 - val_loss: 55.9434\n",
      "Epoch 632/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.1330 - val_loss: 55.9263\n",
      "Epoch 633/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.1198 - val_loss: 55.9004\n",
      "Epoch 634/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 51.1089 - val_loss: 55.8577\n",
      "Epoch 635/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 51.1034 - val_loss: 55.8135\n",
      "Epoch 636/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.0963 - val_loss: 55.7810\n",
      "Epoch 637/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.0708 - val_loss: 55.6956\n",
      "Epoch 638/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.0581 - val_loss: 55.5869\n",
      "Epoch 639/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.0582 - val_loss: 55.4922\n",
      "Epoch 640/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 51.0482 - val_loss: 55.4205\n",
      "Epoch 641/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 51.0391 - val_loss: 55.3429\n",
      "Epoch 642/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 51.0347 - val_loss: 55.2811\n",
      "Epoch 643/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.0251 - val_loss: 55.2562\n",
      "Epoch 644/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.0141 - val_loss: 55.2450\n",
      "Epoch 645/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 51.0036 - val_loss: 55.2640\n",
      "Epoch 646/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 50.9799 - val_loss: 55.2770\n",
      "Epoch 647/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.9666 - val_loss: 55.3204\n",
      "Epoch 648/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.9509 - val_loss: 55.3532\n",
      "Epoch 649/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.9296 - val_loss: 55.3761\n",
      "Epoch 650/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 50.9198 - val_loss: 55.3775\n",
      "Epoch 651/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.9030 - val_loss: 55.3626\n",
      "Epoch 652/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.8945 - val_loss: 55.3546\n",
      "Epoch 653/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.8828 - val_loss: 55.3114\n",
      "Epoch 654/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.8653 - val_loss: 55.2236\n",
      "Epoch 655/1500\n",
      "87/87 [==============================] - ETA: 0s - loss: 55.83 - 0s 80us/step - loss: 50.8471 - val_loss: 55.1215\n",
      "Epoch 656/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.8401 - val_loss: 55.0158\n",
      "Epoch 657/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.8401 - val_loss: 54.9046\n",
      "Epoch 658/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.8392 - val_loss: 54.8178\n",
      "Epoch 659/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.8283 - val_loss: 54.7541\n",
      "Epoch 660/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.8257 - val_loss: 54.6903\n",
      "Epoch 661/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.8265 - val_loss: 54.6347\n",
      "Epoch 662/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.8085 - val_loss: 54.6320\n",
      "Epoch 663/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 50.7885 - val_loss: 54.6390\n",
      "Epoch 664/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 50.7821 - val_loss: 54.6425\n",
      "Epoch 665/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.7583 - val_loss: 54.6044\n",
      "Epoch 666/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.7417 - val_loss: 54.5577\n",
      "Epoch 667/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.7306 - val_loss: 54.5132\n",
      "Epoch 668/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 50.7210 - val_loss: 54.4640\n",
      "Epoch 669/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 50.7229 - val_loss: 54.4230\n",
      "Epoch 670/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 50.7017 - val_loss: 54.4396\n",
      "Epoch 671/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 50.6939 - val_loss: 54.4636\n",
      "Epoch 672/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.6678 - val_loss: 54.4721\n",
      "Epoch 673/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.6501 - val_loss: 54.5264\n",
      "Epoch 674/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 50.6415 - val_loss: 54.5863\n",
      "Epoch 675/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.6152 - val_loss: 54.6137\n",
      "Epoch 676/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 50.6026 - val_loss: 54.6384\n",
      "Epoch 677/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.5897 - val_loss: 54.6772\n",
      "Epoch 678/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.6100 - val_loss: 54.6843\n",
      "Epoch 679/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.5999 - val_loss: 54.5898\n",
      "Epoch 680/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.5589 - val_loss: 54.4819\n",
      "Epoch 681/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 50.5476 - val_loss: 54.3850\n",
      "Epoch 682/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.5307 - val_loss: 54.2995\n",
      "Epoch 683/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 50.5250 - val_loss: 54.2168\n",
      "Epoch 684/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.5314 - val_loss: 54.1740\n",
      "Epoch 685/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 50.5128 - val_loss: 54.1774\n",
      "Epoch 686/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.4972 - val_loss: 54.1640\n",
      "Epoch 687/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.4845 - val_loss: 54.1445\n",
      "Epoch 688/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 50.4703 - val_loss: 54.1172\n",
      "Epoch 689/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.4626 - val_loss: 54.1001\n",
      "Epoch 690/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.4500 - val_loss: 54.0754\n",
      "Epoch 691/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 50.4387 - val_loss: 54.0688\n",
      "Epoch 692/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 50.4312 - val_loss: 54.0811\n",
      "Epoch 693/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.4188 - val_loss: 54.0679\n",
      "Epoch 694/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 50.4035 - val_loss: 54.0448\n",
      "Epoch 695/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 50.3970 - val_loss: 54.0272\n",
      "Epoch 696/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.3911 - val_loss: 54.0292\n",
      "Epoch 697/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 50.3733 - val_loss: 53.9861\n",
      "Epoch 698/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 50.3682 - val_loss: 53.9268\n",
      "Epoch 699/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.3491 - val_loss: 53.9063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 700/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 50.3357 - val_loss: 53.9047\n",
      "Epoch 701/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 50.3308 - val_loss: 53.8927\n",
      "Epoch 702/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 50.3124 - val_loss: 53.8509\n",
      "Epoch 703/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.3022 - val_loss: 53.8183\n",
      "Epoch 704/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 50.2896 - val_loss: 53.7877\n",
      "Epoch 705/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.2766 - val_loss: 53.7480\n",
      "Epoch 706/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 50.2657 - val_loss: 53.6995\n",
      "Epoch 707/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 50.2537 - val_loss: 53.6371\n",
      "Epoch 708/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 50.2427 - val_loss: 53.5781\n",
      "Epoch 709/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.2313 - val_loss: 53.5369\n",
      "Epoch 710/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.2203 - val_loss: 53.5090\n",
      "Epoch 711/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 50.2056 - val_loss: 53.4596\n",
      "Epoch 712/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 50.1937 - val_loss: 53.4198\n",
      "Epoch 713/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.1825 - val_loss: 53.4001\n",
      "Epoch 714/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.1726 - val_loss: 53.3836\n",
      "Epoch 715/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 50.1582 - val_loss: 53.3638\n",
      "Epoch 716/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.1450 - val_loss: 53.3584\n",
      "Epoch 717/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 50.1394 - val_loss: 53.3306\n",
      "Epoch 718/1500\n",
      "87/87 [==============================] - 0s 218us/step - loss: 50.1205 - val_loss: 53.2632\n",
      "Epoch 719/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 50.1033 - val_loss: 53.1707\n",
      "Epoch 720/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.0946 - val_loss: 53.0868\n",
      "Epoch 721/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.0949 - val_loss: 53.0145\n",
      "Epoch 722/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.0769 - val_loss: 52.9958\n",
      "Epoch 723/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.0656 - val_loss: 53.0059\n",
      "Epoch 724/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 50.0405 - val_loss: 53.0383\n",
      "Epoch 725/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.0280 - val_loss: 53.1086\n",
      "Epoch 726/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.0182 - val_loss: 53.1489\n",
      "Epoch 727/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 50.0218 - val_loss: 53.1719\n",
      "Epoch 728/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.0223 - val_loss: 53.1531\n",
      "Epoch 729/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 50.0118 - val_loss: 53.0960\n",
      "Epoch 730/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.9920 - val_loss: 53.0249\n",
      "Epoch 731/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.9678 - val_loss: 52.9262\n",
      "Epoch 732/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.9480 - val_loss: 52.8053\n",
      "Epoch 733/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 49.9301 - val_loss: 52.6928\n",
      "Epoch 734/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 49.9326 - val_loss: 52.6025\n",
      "Epoch 735/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.9398 - val_loss: 52.5610\n",
      "Epoch 736/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.9230 - val_loss: 52.5830\n",
      "Epoch 737/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 49.8952 - val_loss: 52.6373\n",
      "Epoch 738/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.8683 - val_loss: 52.7074\n",
      "Epoch 739/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.8830 - val_loss: 52.7827\n",
      "Epoch 740/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 49.8782 - val_loss: 52.7960\n",
      "Epoch 741/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 49.8794 - val_loss: 52.7652\n",
      "Epoch 742/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 49.8635 - val_loss: 52.6623\n",
      "Epoch 743/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.8596 - val_loss: 52.5074\n",
      "Epoch 744/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 49.8090 - val_loss: 52.3809\n",
      "Epoch 745/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 49.8008 - val_loss: 52.2156\n",
      "Epoch 746/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 49.8251 - val_loss: 52.0998\n",
      "Epoch 747/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.8259 - val_loss: 52.0346\n",
      "Epoch 748/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 49.8155 - val_loss: 51.9581\n",
      "Epoch 749/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.8322 - val_loss: 51.9302\n",
      "Epoch 750/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 49.7867 - val_loss: 52.0022\n",
      "Epoch 751/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.7516 - val_loss: 52.1286\n",
      "Epoch 752/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.7251 - val_loss: 52.2423\n",
      "Epoch 753/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 49.7395 - val_loss: 52.3537\n",
      "Epoch 754/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 49.7332 - val_loss: 52.4076\n",
      "Epoch 755/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 49.7492 - val_loss: 52.4011\n",
      "Epoch 756/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 49.7292 - val_loss: 52.3202\n",
      "Epoch 757/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 49.7248 - val_loss: 52.2102\n",
      "Epoch 758/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 49.6873 - val_loss: 52.1372\n",
      "Epoch 759/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.6737 - val_loss: 52.0739\n",
      "Epoch 760/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.6529 - val_loss: 52.0173\n",
      "Epoch 761/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 49.6370 - val_loss: 51.9376\n",
      "Epoch 762/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.6236 - val_loss: 51.8324\n",
      "Epoch 763/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.6324 - val_loss: 51.7360\n",
      "Epoch 764/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 49.6148 - val_loss: 51.6992\n",
      "Epoch 765/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 49.6002 - val_loss: 51.7055\n",
      "Epoch 766/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.5928 - val_loss: 51.7206\n",
      "Epoch 767/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 49.5756 - val_loss: 51.7244\n",
      "Epoch 768/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 49.5658 - val_loss: 51.7296\n",
      "Epoch 769/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 49.5625 - val_loss: 51.7265\n",
      "Epoch 770/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.5503 - val_loss: 51.6936\n",
      "Epoch 771/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 49.5363 - val_loss: 51.6815\n",
      "Epoch 772/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.5275 - val_loss: 51.6706\n",
      "Epoch 773/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 49.5197 - val_loss: 51.6382\n",
      "Epoch 774/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 49.5040 - val_loss: 51.5708\n",
      "Epoch 775/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.5016 - val_loss: 51.5238\n",
      "Epoch 776/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 49.4878 - val_loss: 51.5246\n",
      "Epoch 777/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.4739 - val_loss: 51.5531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 778/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.4580 - val_loss: 51.5997\n",
      "Epoch 779/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.4583 - val_loss: 51.6215\n",
      "Epoch 780/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 49.4518 - val_loss: 51.6174\n",
      "Epoch 781/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.4482 - val_loss: 51.5903\n",
      "Epoch 782/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.4384 - val_loss: 51.5149\n",
      "Epoch 783/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.4177 - val_loss: 51.3837\n",
      "Epoch 784/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 49.3876 - val_loss: 51.2649\n",
      "Epoch 785/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 49.3916 - val_loss: 51.1467\n",
      "Epoch 786/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 49.3847 - val_loss: 51.0790\n",
      "Epoch 787/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 49.3745 - val_loss: 51.0355\n",
      "Epoch 788/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.3623 - val_loss: 50.9640\n",
      "Epoch 789/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 49.3540 - val_loss: 50.8990\n",
      "Epoch 790/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.3540 - val_loss: 50.8594\n",
      "Epoch 791/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.3399 - val_loss: 50.8613\n",
      "Epoch 792/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.3233 - val_loss: 50.8874\n",
      "Epoch 793/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.2987 - val_loss: 50.9353\n",
      "Epoch 794/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.2798 - val_loss: 50.9998\n",
      "Epoch 795/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 49.2852 - val_loss: 51.0598\n",
      "Epoch 796/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 49.2748 - val_loss: 51.0813\n",
      "Epoch 797/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.2707 - val_loss: 51.0735\n",
      "Epoch 798/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.2652 - val_loss: 51.0614\n",
      "Epoch 799/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 49.2575 - val_loss: 51.0257\n",
      "Epoch 800/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.2477 - val_loss: 50.9581\n",
      "Epoch 801/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.2232 - val_loss: 50.8611\n",
      "Epoch 802/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 49.2149 - val_loss: 50.7569\n",
      "Epoch 803/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.1825 - val_loss: 50.6873\n",
      "Epoch 804/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 49.1741 - val_loss: 50.5978\n",
      "Epoch 805/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.1571 - val_loss: 50.5234\n",
      "Epoch 806/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.1565 - val_loss: 50.4425\n",
      "Epoch 807/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 49.1529 - val_loss: 50.4294\n",
      "Epoch 808/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.1336 - val_loss: 50.4798\n",
      "Epoch 809/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.1388 - val_loss: 50.5448\n",
      "Epoch 810/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.1154 - val_loss: 50.5368\n",
      "Epoch 811/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 49.1002 - val_loss: 50.4752\n",
      "Epoch 812/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.0790 - val_loss: 50.3562\n",
      "Epoch 813/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 49.0721 - val_loss: 50.2271\n",
      "Epoch 814/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.0992 - val_loss: 50.1017\n",
      "Epoch 815/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.1151 - val_loss: 50.0248\n",
      "Epoch 816/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.1248 - val_loss: 49.9913\n",
      "Epoch 817/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.1215 - val_loss: 50.0007\n",
      "Epoch 818/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 49.0981 - val_loss: 50.0587\n",
      "Epoch 819/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.0463 - val_loss: 50.2126\n",
      "Epoch 820/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.9885 - val_loss: 50.4060\n",
      "Epoch 821/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.0282 - val_loss: 50.5756\n",
      "Epoch 822/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.0785 - val_loss: 50.6594\n",
      "Epoch 823/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 49.0959 - val_loss: 50.6198\n",
      "Epoch 824/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 49.0703 - val_loss: 50.5126\n",
      "Epoch 825/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 49.0425 - val_loss: 50.3262\n",
      "Epoch 826/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 48.9599 - val_loss: 50.1536\n",
      "Epoch 827/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.9427 - val_loss: 49.9796\n",
      "Epoch 828/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.9689 - val_loss: 49.8555\n",
      "Epoch 829/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.9652 - val_loss: 49.8038\n",
      "Epoch 830/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.9643 - val_loss: 49.7784\n",
      "Epoch 831/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 48.9529 - val_loss: 49.7785\n",
      "Epoch 832/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.9241 - val_loss: 49.7915\n",
      "Epoch 833/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 48.8947 - val_loss: 49.8612\n",
      "Epoch 834/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.8679 - val_loss: 49.9321\n",
      "Epoch 835/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.8794 - val_loss: 50.0213\n",
      "Epoch 836/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.8957 - val_loss: 50.0833\n",
      "Epoch 837/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.9222 - val_loss: 50.0653\n",
      "Epoch 838/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.8981 - val_loss: 49.9578\n",
      "Epoch 839/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.8565 - val_loss: 49.8206\n",
      "Epoch 840/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.8364 - val_loss: 49.6722\n",
      "Epoch 841/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 48.8082 - val_loss: 49.5380\n",
      "Epoch 842/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 48.8437 - val_loss: 49.4390\n",
      "Epoch 843/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.8330 - val_loss: 49.4183\n",
      "Epoch 844/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.8167 - val_loss: 49.4163\n",
      "Epoch 845/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.7969 - val_loss: 49.4604\n",
      "Epoch 846/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.7791 - val_loss: 49.5128\n",
      "Epoch 847/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.7572 - val_loss: 49.5401\n",
      "Epoch 848/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.7622 - val_loss: 49.5532\n",
      "Epoch 849/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 48.7579 - val_loss: 49.5325\n",
      "Epoch 850/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.7544 - val_loss: 49.5067\n",
      "Epoch 851/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.7320 - val_loss: 49.4316\n",
      "Epoch 852/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.7187 - val_loss: 49.3487\n",
      "Epoch 853/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.7298 - val_loss: 49.2929\n",
      "Epoch 854/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.7060 - val_loss: 49.3197\n",
      "Epoch 855/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 48.7017 - val_loss: 49.3282\n",
      "Epoch 856/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 92us/step - loss: 48.6885 - val_loss: 49.2960\n",
      "Epoch 857/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.6830 - val_loss: 49.2800\n",
      "Epoch 858/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 48.6728 - val_loss: 49.2877\n",
      "Epoch 859/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.6702 - val_loss: 49.2664\n",
      "Epoch 860/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.6624 - val_loss: 49.2321\n",
      "Epoch 861/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 48.6547 - val_loss: 49.2069\n",
      "Epoch 862/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.6419 - val_loss: 49.1724\n",
      "Epoch 863/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.6341 - val_loss: 49.1200\n",
      "Epoch 864/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.6302 - val_loss: 49.0973\n",
      "Epoch 865/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.6194 - val_loss: 49.1002\n",
      "Epoch 866/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.6120 - val_loss: 49.0623\n",
      "Epoch 867/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 48.6068 - val_loss: 49.0036\n",
      "Epoch 868/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.6028 - val_loss: 48.9783\n",
      "Epoch 869/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.5946 - val_loss: 48.9818\n",
      "Epoch 870/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.5855 - val_loss: 49.0125\n",
      "Epoch 871/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.5807 - val_loss: 49.0378\n",
      "Epoch 872/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.5843 - val_loss: 49.0464\n",
      "Epoch 873/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 48.5784 - val_loss: 49.0991\n",
      "Epoch 874/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.5832 - val_loss: 49.1734\n",
      "Epoch 875/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.6052 - val_loss: 49.2055\n",
      "Epoch 876/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.6165 - val_loss: 49.1752\n",
      "Epoch 877/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.6033 - val_loss: 49.1005\n",
      "Epoch 878/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.5762 - val_loss: 49.0255\n",
      "Epoch 879/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 48.5504 - val_loss: 48.9653\n",
      "Epoch 880/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.5309 - val_loss: 48.9001\n",
      "Epoch 881/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 48.5229 - val_loss: 48.7822\n",
      "Epoch 882/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.5082 - val_loss: 48.6858\n",
      "Epoch 883/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 48.5118 - val_loss: 48.6004\n",
      "Epoch 884/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.5177 - val_loss: 48.5572\n",
      "Epoch 885/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.5150 - val_loss: 48.5530\n",
      "Epoch 886/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.4933 - val_loss: 48.5264\n",
      "Epoch 887/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 48.4850 - val_loss: 48.5100\n",
      "Epoch 888/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 48.4767 - val_loss: 48.4938\n",
      "Epoch 889/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.4761 - val_loss: 48.4769\n",
      "Epoch 890/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.4674 - val_loss: 48.4463\n",
      "Epoch 891/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.4674 - val_loss: 48.4425\n",
      "Epoch 892/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.4563 - val_loss: 48.4103\n",
      "Epoch 893/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.4488 - val_loss: 48.4144\n",
      "Epoch 894/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.4359 - val_loss: 48.3743\n",
      "Epoch 895/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 48.4355 - val_loss: 48.3415\n",
      "Epoch 896/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 48.4340 - val_loss: 48.3661\n",
      "Epoch 897/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.4073 - val_loss: 48.4750\n",
      "Epoch 898/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.4324 - val_loss: 48.6119\n",
      "Epoch 899/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.4812 - val_loss: 48.6711\n",
      "Epoch 900/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.5064 - val_loss: 48.6206\n",
      "Epoch 901/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.4508 - val_loss: 48.4810\n",
      "Epoch 902/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.4133 - val_loss: 48.3155\n",
      "Epoch 903/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.3767 - val_loss: 48.1411\n",
      "Epoch 904/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 48.3908 - val_loss: 47.9626\n",
      "Epoch 905/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.4258 - val_loss: 47.8574\n",
      "Epoch 906/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.4322 - val_loss: 47.8571\n",
      "Epoch 907/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.4209 - val_loss: 47.8868\n",
      "Epoch 908/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 48.3917 - val_loss: 47.9130\n",
      "Epoch 909/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.3774 - val_loss: 47.9502\n",
      "Epoch 910/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.3682 - val_loss: 47.9721\n",
      "Epoch 911/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 48.3557 - val_loss: 47.9794\n",
      "Epoch 912/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 48.3484 - val_loss: 47.9995\n",
      "Epoch 913/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.3431 - val_loss: 48.0247\n",
      "Epoch 914/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.3367 - val_loss: 48.0357\n",
      "Epoch 915/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.3341 - val_loss: 48.0536\n",
      "Epoch 916/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.3473 - val_loss: 48.0611\n",
      "Epoch 917/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.3261 - val_loss: 48.0059\n",
      "Epoch 918/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.3342 - val_loss: 47.9830\n",
      "Epoch 919/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.3149 - val_loss: 48.0169\n",
      "Epoch 920/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.3112 - val_loss: 48.0420\n",
      "Epoch 921/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 48.3118 - val_loss: 48.0385\n",
      "Epoch 922/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.3007 - val_loss: 47.9854\n",
      "Epoch 923/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.2900 - val_loss: 47.8849\n",
      "Epoch 924/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.3068 - val_loss: 47.7643\n",
      "Epoch 925/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.3003 - val_loss: 47.7461\n",
      "Epoch 926/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.3075 - val_loss: 47.7347\n",
      "Epoch 927/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 48.2797 - val_loss: 47.7599\n",
      "Epoch 928/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 48.2689 - val_loss: 47.7867\n",
      "Epoch 929/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.2712 - val_loss: 47.8360\n",
      "Epoch 930/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.2639 - val_loss: 47.9402\n",
      "Epoch 931/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.2669 - val_loss: 48.0235\n",
      "Epoch 932/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.2708 - val_loss: 48.0928\n",
      "Epoch 933/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.2827 - val_loss: 48.1658\n",
      "Epoch 934/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 80us/step - loss: 48.2937 - val_loss: 48.2153\n",
      "Epoch 935/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 48.3079 - val_loss: 48.2301\n",
      "Epoch 936/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.3147 - val_loss: 48.2144\n",
      "Epoch 937/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 48.3116 - val_loss: 48.1774\n",
      "Epoch 938/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.2945 - val_loss: 48.1400\n",
      "Epoch 939/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.2788 - val_loss: 48.0690\n",
      "Epoch 940/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 48.2808 - val_loss: 47.9770\n",
      "Epoch 941/1500\n",
      "87/87 [==============================] - 0s 195us/step - loss: 48.2476 - val_loss: 47.9022\n",
      "Epoch 942/1500\n",
      "87/87 [==============================] - 0s 161us/step - loss: 48.2386 - val_loss: 47.7844\n",
      "Epoch 943/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 48.2195 - val_loss: 47.6904\n",
      "Epoch 944/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 48.2045 - val_loss: 47.5905\n",
      "Epoch 945/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 48.1992 - val_loss: 47.5405\n",
      "Epoch 946/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 48.2028 - val_loss: 47.5161\n",
      "Epoch 947/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 48.2391 - val_loss: 47.4927\n",
      "Epoch 948/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 48.2215 - val_loss: 47.4940\n",
      "Epoch 949/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.1950 - val_loss: 47.5142\n",
      "Epoch 950/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 48.1822 - val_loss: 47.5668\n",
      "Epoch 951/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 48.1779 - val_loss: 47.6168\n",
      "Epoch 952/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 48.1664 - val_loss: 47.7243\n",
      "Epoch 953/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 48.1741 - val_loss: 47.8532\n",
      "Epoch 954/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 48.1895 - val_loss: 47.9820\n",
      "Epoch 955/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.2432 - val_loss: 48.0617\n",
      "Epoch 956/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.2666 - val_loss: 47.9854\n",
      "Epoch 957/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.2095 - val_loss: 47.8069\n",
      "Epoch 958/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.2027 - val_loss: 47.6033\n",
      "Epoch 959/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.1520 - val_loss: 47.4296\n",
      "Epoch 960/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 48.1391 - val_loss: 47.2599\n",
      "Epoch 961/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 48.1397 - val_loss: 47.1174\n",
      "Epoch 962/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.1312 - val_loss: 47.0717\n",
      "Epoch 963/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.1490 - val_loss: 47.0286\n",
      "Epoch 964/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.1568 - val_loss: 47.0072\n",
      "Epoch 965/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.1494 - val_loss: 46.9924\n",
      "Epoch 966/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.1382 - val_loss: 46.9847\n",
      "Epoch 967/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.1187 - val_loss: 46.9869\n",
      "Epoch 968/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 48.1220 - val_loss: 47.0235\n",
      "Epoch 969/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 48.1123 - val_loss: 47.0198\n",
      "Epoch 970/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.1055 - val_loss: 47.0202\n",
      "Epoch 971/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.0990 - val_loss: 47.0313\n",
      "Epoch 972/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.0915 - val_loss: 47.0323\n",
      "Epoch 973/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.0910 - val_loss: 47.0463\n",
      "Epoch 974/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.0764 - val_loss: 47.0647\n",
      "Epoch 975/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 48.0750 - val_loss: 47.0907\n",
      "Epoch 976/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 48.0660 - val_loss: 47.1134\n",
      "Epoch 977/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 48.0648 - val_loss: 47.1396\n",
      "Epoch 978/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.0627 - val_loss: 47.1505\n",
      "Epoch 979/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.0610 - val_loss: 47.1491\n",
      "Epoch 980/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.0557 - val_loss: 47.1514\n",
      "Epoch 981/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.0561 - val_loss: 47.1434\n",
      "Epoch 982/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.0488 - val_loss: 47.1155\n",
      "Epoch 983/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 48.0463 - val_loss: 47.0931\n",
      "Epoch 984/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.0409 - val_loss: 47.0887\n",
      "Epoch 985/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 48.0337 - val_loss: 47.0753\n",
      "Epoch 986/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.0342 - val_loss: 47.0508\n",
      "Epoch 987/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.0258 - val_loss: 47.0170\n",
      "Epoch 988/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.0204 - val_loss: 46.9708\n",
      "Epoch 989/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.0187 - val_loss: 46.9095\n",
      "Epoch 990/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 48.0176 - val_loss: 46.8484\n",
      "Epoch 991/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.0089 - val_loss: 46.7881\n",
      "Epoch 992/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.0106 - val_loss: 46.7375\n",
      "Epoch 993/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.0212 - val_loss: 46.6978\n",
      "Epoch 994/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.0205 - val_loss: 46.6723\n",
      "Epoch 995/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.0050 - val_loss: 46.6465\n",
      "Epoch 996/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 48.0034 - val_loss: 46.6158\n",
      "Epoch 997/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.0017 - val_loss: 46.5949\n",
      "Epoch 998/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 48.0151 - val_loss: 46.5819\n",
      "Epoch 999/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.9983 - val_loss: 46.5480\n",
      "Epoch 1000/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.9914 - val_loss: 46.5281\n",
      "Epoch 1001/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.9859 - val_loss: 46.5192\n",
      "Epoch 1002/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.9824 - val_loss: 46.5120\n",
      "Epoch 1003/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.9743 - val_loss: 46.5067\n",
      "Epoch 1004/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.9712 - val_loss: 46.5152\n",
      "Epoch 1005/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.9620 - val_loss: 46.5369\n",
      "Epoch 1006/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 47.9546 - val_loss: 46.5864\n",
      "Epoch 1007/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.9607 - val_loss: 46.6174\n",
      "Epoch 1008/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.9544 - val_loss: 46.5920\n",
      "Epoch 1009/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.9530 - val_loss: 46.5649\n",
      "Epoch 1010/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.9409 - val_loss: 46.5700\n",
      "Epoch 1011/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 47.9417 - val_loss: 46.5639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1012/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.9327 - val_loss: 46.5614\n",
      "Epoch 1013/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.9347 - val_loss: 46.5605\n",
      "Epoch 1014/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.9255 - val_loss: 46.5697\n",
      "Epoch 1015/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.9131 - val_loss: 46.5939\n",
      "Epoch 1016/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.9148 - val_loss: 46.6678\n",
      "Epoch 1017/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.9563 - val_loss: 46.7592\n",
      "Epoch 1018/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.9572 - val_loss: 46.7751\n",
      "Epoch 1019/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.9582 - val_loss: 46.7539\n",
      "Epoch 1020/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.9560 - val_loss: 46.7104\n",
      "Epoch 1021/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.9395 - val_loss: 46.6604\n",
      "Epoch 1022/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 47.9276 - val_loss: 46.6283\n",
      "Epoch 1023/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.9004 - val_loss: 46.5878\n",
      "Epoch 1024/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.8663 - val_loss: 46.5339\n",
      "Epoch 1025/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.9063 - val_loss: 46.4823\n",
      "Epoch 1026/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.9533 - val_loss: 46.4523\n",
      "Epoch 1027/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 47.9798 - val_loss: 46.4281\n",
      "Epoch 1028/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.9922 - val_loss: 46.3937\n",
      "Epoch 1029/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.9706 - val_loss: 46.3822\n",
      "Epoch 1030/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.9027 - val_loss: 46.3987\n",
      "Epoch 1031/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.8632 - val_loss: 46.4226\n",
      "Epoch 1032/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.9070 - val_loss: 46.4891\n",
      "Epoch 1033/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.9036 - val_loss: 46.5229\n",
      "Epoch 1034/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.9301 - val_loss: 46.5284\n",
      "Epoch 1035/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.9198 - val_loss: 46.4571\n",
      "Epoch 1036/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.9031 - val_loss: 46.3498\n",
      "Epoch 1037/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 47.8828 - val_loss: 46.2771\n",
      "Epoch 1038/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.8683 - val_loss: 46.2352\n",
      "Epoch 1039/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.8548 - val_loss: 46.2104\n",
      "Epoch 1040/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.8372 - val_loss: 46.1992\n",
      "Epoch 1041/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.8449 - val_loss: 46.1736\n",
      "Epoch 1042/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.8531 - val_loss: 46.1326\n",
      "Epoch 1043/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.8570 - val_loss: 46.0910\n",
      "Epoch 1044/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.8455 - val_loss: 46.0630\n",
      "Epoch 1045/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.8379 - val_loss: 46.0390\n",
      "Epoch 1046/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.8334 - val_loss: 46.0099\n",
      "Epoch 1047/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.8539 - val_loss: 45.9778\n",
      "Epoch 1048/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.8273 - val_loss: 45.9699\n",
      "Epoch 1049/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.8218 - val_loss: 45.9641\n",
      "Epoch 1050/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 47.8209 - val_loss: 45.9623\n",
      "Epoch 1051/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.8423 - val_loss: 45.9668\n",
      "Epoch 1052/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.8094 - val_loss: 45.9770\n",
      "Epoch 1053/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.8438 - val_loss: 45.9955\n",
      "Epoch 1054/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.8128 - val_loss: 45.9954\n",
      "Epoch 1055/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.8025 - val_loss: 45.9946\n",
      "Epoch 1056/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.8072 - val_loss: 45.9932\n",
      "Epoch 1057/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.8065 - val_loss: 45.9929\n",
      "Epoch 1058/1500\n",
      "87/87 [==============================] - ETA: 0s - loss: 38.08 - 0s 80us/step - loss: 47.7885 - val_loss: 45.9768\n",
      "Epoch 1059/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.8124 - val_loss: 45.9712\n",
      "Epoch 1060/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.8034 - val_loss: 45.9838\n",
      "Epoch 1061/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.7976 - val_loss: 45.9944\n",
      "Epoch 1062/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.7770 - val_loss: 46.0033\n",
      "Epoch 1063/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.7748 - val_loss: 46.0308\n",
      "Epoch 1064/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.7869 - val_loss: 46.0492\n",
      "Epoch 1065/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.7726 - val_loss: 46.0488\n",
      "Epoch 1066/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.7606 - val_loss: 46.0491\n",
      "Epoch 1067/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.7682 - val_loss: 46.0601\n",
      "Epoch 1068/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.7714 - val_loss: 46.0779\n",
      "Epoch 1069/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.7715 - val_loss: 46.1000\n",
      "Epoch 1070/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.7547 - val_loss: 46.1011\n",
      "Epoch 1071/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.7534 - val_loss: 46.0934\n",
      "Epoch 1072/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.7602 - val_loss: 46.0845\n",
      "Epoch 1073/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.7520 - val_loss: 46.0640\n",
      "Epoch 1074/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.7384 - val_loss: 46.0234\n",
      "Epoch 1075/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.7655 - val_loss: 45.9871\n",
      "Epoch 1076/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.7695 - val_loss: 45.9606\n",
      "Epoch 1077/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.7653 - val_loss: 45.9162\n",
      "Epoch 1078/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.7632 - val_loss: 45.8647\n",
      "Epoch 1079/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.7618 - val_loss: 45.8242\n",
      "Epoch 1080/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.7465 - val_loss: 45.7941\n",
      "Epoch 1081/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.7377 - val_loss: 45.7714\n",
      "Epoch 1082/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.7201 - val_loss: 45.7527\n",
      "Epoch 1083/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.7289 - val_loss: 45.7512\n",
      "Epoch 1084/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.7128 - val_loss: 45.7368\n",
      "Epoch 1085/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.7147 - val_loss: 45.7307\n",
      "Epoch 1086/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.7044 - val_loss: 45.7455\n",
      "Epoch 1087/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.7036 - val_loss: 45.7509\n",
      "Epoch 1088/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.6971 - val_loss: 45.7374\n",
      "Epoch 1089/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 80us/step - loss: 47.6975 - val_loss: 45.7252\n",
      "Epoch 1090/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 47.7114 - val_loss: 45.7276\n",
      "Epoch 1091/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.7109 - val_loss: 45.7289\n",
      "Epoch 1092/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.6919 - val_loss: 45.7123\n",
      "Epoch 1093/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.6892 - val_loss: 45.6983\n",
      "Epoch 1094/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.6874 - val_loss: 45.7013\n",
      "Epoch 1095/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.6838 - val_loss: 45.6939\n",
      "Epoch 1096/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.6843 - val_loss: 45.6827\n",
      "Epoch 1097/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.6763 - val_loss: 45.6709\n",
      "Epoch 1098/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.6684 - val_loss: 45.6527\n",
      "Epoch 1099/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.6632 - val_loss: 45.6404\n",
      "Epoch 1100/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.7034 - val_loss: 45.6308\n",
      "Epoch 1101/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.7018 - val_loss: 45.6223\n",
      "Epoch 1102/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.6772 - val_loss: 45.6030\n",
      "Epoch 1103/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.6774 - val_loss: 45.5818\n",
      "Epoch 1104/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.6590 - val_loss: 45.5520\n",
      "Epoch 1105/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 47.6566 - val_loss: 45.5257\n",
      "Epoch 1106/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.6527 - val_loss: 45.5097\n",
      "Epoch 1107/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.6433 - val_loss: 45.5064\n",
      "Epoch 1108/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.6420 - val_loss: 45.4984\n",
      "Epoch 1109/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.6504 - val_loss: 45.4828\n",
      "Epoch 1110/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.6509 - val_loss: 45.4703\n",
      "Epoch 1111/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.6504 - val_loss: 45.4597\n",
      "Epoch 1112/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.6462 - val_loss: 45.4337\n",
      "Epoch 1113/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.6391 - val_loss: 45.4098\n",
      "Epoch 1114/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.6298 - val_loss: 45.3913\n",
      "Epoch 1115/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.6236 - val_loss: 45.3619\n",
      "Epoch 1116/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.6121 - val_loss: 45.3253\n",
      "Epoch 1117/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.6175 - val_loss: 45.2903\n",
      "Epoch 1118/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.6246 - val_loss: 45.2611\n",
      "Epoch 1119/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.6261 - val_loss: 45.2397\n",
      "Epoch 1120/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 47.6197 - val_loss: 45.2234\n",
      "Epoch 1121/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.6061 - val_loss: 45.1920\n",
      "Epoch 1122/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.6069 - val_loss: 45.1572\n",
      "Epoch 1123/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.6096 - val_loss: 45.1336\n",
      "Epoch 1124/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.6035 - val_loss: 45.1311\n",
      "Epoch 1125/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.5985 - val_loss: 45.2041\n",
      "Epoch 1126/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.6043 - val_loss: 45.3183\n",
      "Epoch 1127/1500\n",
      "87/87 [==============================] - 0s 230us/step - loss: 47.6165 - val_loss: 45.3968\n",
      "Epoch 1128/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.6243 - val_loss: 45.4304\n",
      "Epoch 1129/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.6244 - val_loss: 45.4352\n",
      "Epoch 1130/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.6222 - val_loss: 45.4556\n",
      "Epoch 1131/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.6273 - val_loss: 45.4769\n",
      "Epoch 1132/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 47.6187 - val_loss: 45.4409\n",
      "Epoch 1133/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.6102 - val_loss: 45.3898\n",
      "Epoch 1134/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.5987 - val_loss: 45.3394\n",
      "Epoch 1135/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.5886 - val_loss: 45.2834\n",
      "Epoch 1136/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.5929 - val_loss: 45.2457\n",
      "Epoch 1137/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.5678 - val_loss: 45.2318\n",
      "Epoch 1138/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.5598 - val_loss: 45.1868\n",
      "Epoch 1139/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.5487 - val_loss: 45.1569\n",
      "Epoch 1140/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.5467 - val_loss: 45.1171\n",
      "Epoch 1141/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.5380 - val_loss: 45.0831\n",
      "Epoch 1142/1500\n",
      "87/87 [==============================] - ETA: 0s - loss: 49.38 - 0s 81us/step - loss: 47.5583 - val_loss: 45.0439\n",
      "Epoch 1143/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.5752 - val_loss: 45.0139\n",
      "Epoch 1144/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.5877 - val_loss: 44.9795\n",
      "Epoch 1145/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 47.5793 - val_loss: 44.9452\n",
      "Epoch 1146/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 47.5565 - val_loss: 44.9046\n",
      "Epoch 1147/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.5295 - val_loss: 44.9745\n",
      "Epoch 1148/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.5196 - val_loss: 45.0395\n",
      "Epoch 1149/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.5303 - val_loss: 45.0881\n",
      "Epoch 1150/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 47.5292 - val_loss: 45.1037\n",
      "Epoch 1151/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.5324 - val_loss: 45.1164\n",
      "Epoch 1152/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.5301 - val_loss: 45.1180\n",
      "Epoch 1153/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.5276 - val_loss: 45.0813\n",
      "Epoch 1154/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.5141 - val_loss: 44.9890\n",
      "Epoch 1155/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.5062 - val_loss: 44.8908\n",
      "Epoch 1156/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.5065 - val_loss: 44.8123\n",
      "Epoch 1157/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.4905 - val_loss: 44.7758\n",
      "Epoch 1158/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.5002 - val_loss: 44.7665\n",
      "Epoch 1159/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.4958 - val_loss: 44.8151\n",
      "Epoch 1160/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.4827 - val_loss: 44.8636\n",
      "Epoch 1161/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.4966 - val_loss: 44.9276\n",
      "Epoch 1162/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.4836 - val_loss: 44.9585\n",
      "Epoch 1163/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.4683 - val_loss: 45.0425\n",
      "Epoch 1164/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.4971 - val_loss: 45.0759\n",
      "Epoch 1165/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 47.4858 - val_loss: 45.0477\n",
      "Epoch 1166/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 92us/step - loss: 47.4602 - val_loss: 45.0864\n",
      "Epoch 1167/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.4627 - val_loss: 45.1309\n",
      "Epoch 1168/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.4623 - val_loss: 45.1518\n",
      "Epoch 1169/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.4601 - val_loss: 45.1665\n",
      "Epoch 1170/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.4634 - val_loss: 45.1676\n",
      "Epoch 1171/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.4591 - val_loss: 45.1680\n",
      "Epoch 1172/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.4399 - val_loss: 45.0980\n",
      "Epoch 1173/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.4377 - val_loss: 45.0237\n",
      "Epoch 1174/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 47.4337 - val_loss: 44.9947\n",
      "Epoch 1175/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.4559 - val_loss: 44.9841\n",
      "Epoch 1176/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.4418 - val_loss: 45.0510\n",
      "Epoch 1177/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.4264 - val_loss: 45.1385\n",
      "Epoch 1178/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.4296 - val_loss: 45.2071\n",
      "Epoch 1179/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.4309 - val_loss: 45.2273\n",
      "Epoch 1180/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 47.4190 - val_loss: 45.1986\n",
      "Epoch 1181/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.4109 - val_loss: 45.1333\n",
      "Epoch 1182/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.4278 - val_loss: 45.0474\n",
      "Epoch 1183/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.4482 - val_loss: 45.0552\n",
      "Epoch 1184/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.4423 - val_loss: 45.1323\n",
      "Epoch 1185/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.4277 - val_loss: 45.2116\n",
      "Epoch 1186/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.4213 - val_loss: 45.2424\n",
      "Epoch 1187/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.4048 - val_loss: 45.2011\n",
      "Epoch 1188/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.3994 - val_loss: 45.1350\n",
      "Epoch 1189/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.3962 - val_loss: 45.0740\n",
      "Epoch 1190/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.3875 - val_loss: 45.0202\n",
      "Epoch 1191/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.3827 - val_loss: 44.9594\n",
      "Epoch 1192/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.3849 - val_loss: 44.9254\n",
      "Epoch 1193/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.3794 - val_loss: 44.9255\n",
      "Epoch 1194/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.3708 - val_loss: 44.8918\n",
      "Epoch 1195/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.3672 - val_loss: 44.8388\n",
      "Epoch 1196/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.3705 - val_loss: 44.7831\n",
      "Epoch 1197/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.3639 - val_loss: 44.7602\n",
      "Epoch 1198/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.3734 - val_loss: 44.7497\n",
      "Epoch 1199/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.3528 - val_loss: 44.8070\n",
      "Epoch 1200/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.3617 - val_loss: 44.8796\n",
      "Epoch 1201/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.3729 - val_loss: 44.8937\n",
      "Epoch 1202/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.3520 - val_loss: 44.8399\n",
      "Epoch 1203/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.3428 - val_loss: 44.7900\n",
      "Epoch 1204/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.3379 - val_loss: 44.7243\n",
      "Epoch 1205/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.3432 - val_loss: 44.6796\n",
      "Epoch 1206/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.3337 - val_loss: 44.6930\n",
      "Epoch 1207/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.3196 - val_loss: 44.7551\n",
      "Epoch 1208/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.3212 - val_loss: 44.8179\n",
      "Epoch 1209/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.3339 - val_loss: 44.8817\n",
      "Epoch 1210/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.3542 - val_loss: 44.9346\n",
      "Epoch 1211/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 47.3669 - val_loss: 44.9329\n",
      "Epoch 1212/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 47.3596 - val_loss: 44.8766\n",
      "Epoch 1213/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.3393 - val_loss: 44.8161\n",
      "Epoch 1214/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.3139 - val_loss: 44.7416\n",
      "Epoch 1215/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.3095 - val_loss: 44.6438\n",
      "Epoch 1216/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.3178 - val_loss: 44.5885\n",
      "Epoch 1217/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.3267 - val_loss: 44.5720\n",
      "Epoch 1218/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.3192 - val_loss: 44.5261\n",
      "Epoch 1219/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.3324 - val_loss: 44.4686\n",
      "Epoch 1220/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.3342 - val_loss: 44.3809\n",
      "Epoch 1221/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.3598 - val_loss: 44.3165\n",
      "Epoch 1222/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.3775 - val_loss: 44.3033\n",
      "Epoch 1223/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.3681 - val_loss: 44.3389\n",
      "Epoch 1224/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 47.3335 - val_loss: 44.4666\n",
      "Epoch 1225/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2865 - val_loss: 44.5956\n",
      "Epoch 1226/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.2893 - val_loss: 44.6933\n",
      "Epoch 1227/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.3158 - val_loss: 44.7673\n",
      "Epoch 1228/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.3612 - val_loss: 44.7893\n",
      "Epoch 1229/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.3409 - val_loss: 44.7133\n",
      "Epoch 1230/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.3143 - val_loss: 44.5845\n",
      "Epoch 1231/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2975 - val_loss: 44.4325\n",
      "Epoch 1232/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2824 - val_loss: 44.3478\n",
      "Epoch 1233/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 47.3069 - val_loss: 44.3242\n",
      "Epoch 1234/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.3129 - val_loss: 44.3820\n",
      "Epoch 1235/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.2987 - val_loss: 44.4962\n",
      "Epoch 1236/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2651 - val_loss: 44.5858\n",
      "Epoch 1237/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.2783 - val_loss: 44.6319\n",
      "Epoch 1238/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2757 - val_loss: 44.6122\n",
      "Epoch 1239/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.2645 - val_loss: 44.5846\n",
      "Epoch 1240/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.2845 - val_loss: 44.5900\n",
      "Epoch 1241/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 47.2814 - val_loss: 44.6448\n",
      "Epoch 1242/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2691 - val_loss: 44.6155\n",
      "Epoch 1243/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 69us/step - loss: 47.2610 - val_loss: 44.5660\n",
      "Epoch 1244/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2639 - val_loss: 44.5116\n",
      "Epoch 1245/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2579 - val_loss: 44.4848\n",
      "Epoch 1246/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2618 - val_loss: 44.4623\n",
      "Epoch 1247/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2585 - val_loss: 44.4902\n",
      "Epoch 1248/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2651 - val_loss: 44.5233\n",
      "Epoch 1249/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.2575 - val_loss: 44.5256\n",
      "Epoch 1250/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2518 - val_loss: 44.5592\n",
      "Epoch 1251/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2525 - val_loss: 44.5901\n",
      "Epoch 1252/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2635 - val_loss: 44.6024\n",
      "Epoch 1253/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2607 - val_loss: 44.5527\n",
      "Epoch 1254/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.2533 - val_loss: 44.4725\n",
      "Epoch 1255/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.2532 - val_loss: 44.4277\n",
      "Epoch 1256/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.2510 - val_loss: 44.4262\n",
      "Epoch 1257/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.2537 - val_loss: 44.4517\n",
      "Epoch 1258/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2478 - val_loss: 44.5078\n",
      "Epoch 1259/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.2472 - val_loss: 44.5561\n",
      "Epoch 1260/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2529 - val_loss: 44.5577\n",
      "Epoch 1261/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2479 - val_loss: 44.5148\n",
      "Epoch 1262/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.2484 - val_loss: 44.4807\n",
      "Epoch 1263/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.2429 - val_loss: 44.4901\n",
      "Epoch 1264/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2463 - val_loss: 44.4994\n",
      "Epoch 1265/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2435 - val_loss: 44.5519\n",
      "Epoch 1266/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2468 - val_loss: 44.6354\n",
      "Epoch 1267/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2528 - val_loss: 44.6753\n",
      "Epoch 1268/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2587 - val_loss: 44.6699\n",
      "Epoch 1269/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.2452 - val_loss: 44.6100\n",
      "Epoch 1270/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2491 - val_loss: 44.5412\n",
      "Epoch 1271/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.2477 - val_loss: 44.5108\n",
      "Epoch 1272/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2421 - val_loss: 44.5018\n",
      "Epoch 1273/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.2381 - val_loss: 44.4542\n",
      "Epoch 1274/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.2365 - val_loss: 44.4336\n",
      "Epoch 1275/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2365 - val_loss: 44.4237\n",
      "Epoch 1276/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2399 - val_loss: 44.4340\n",
      "Epoch 1277/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.2269 - val_loss: 44.5154\n",
      "Epoch 1278/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2417 - val_loss: 44.6074\n",
      "Epoch 1279/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2582 - val_loss: 44.6612\n",
      "Epoch 1280/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 47.2615 - val_loss: 44.6811\n",
      "Epoch 1281/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.2598 - val_loss: 44.6893\n",
      "Epoch 1282/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.2498 - val_loss: 44.6810\n",
      "Epoch 1283/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2478 - val_loss: 44.6528\n",
      "Epoch 1284/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2309 - val_loss: 44.6330\n",
      "Epoch 1285/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.2319 - val_loss: 44.5995\n",
      "Epoch 1286/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.2282 - val_loss: 44.5795\n",
      "Epoch 1287/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2277 - val_loss: 44.5474\n",
      "Epoch 1288/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.2253 - val_loss: 44.5091\n",
      "Epoch 1289/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2318 - val_loss: 44.4858\n",
      "Epoch 1290/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.2368 - val_loss: 44.4988\n",
      "Epoch 1291/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.2225 - val_loss: 44.4835\n",
      "Epoch 1292/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.2260 - val_loss: 44.4932\n",
      "Epoch 1293/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2181 - val_loss: 44.4925\n",
      "Epoch 1294/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.2239 - val_loss: 44.5053\n",
      "Epoch 1295/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2153 - val_loss: 44.5577\n",
      "Epoch 1296/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2136 - val_loss: 44.5999\n",
      "Epoch 1297/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2204 - val_loss: 44.6512\n",
      "Epoch 1298/1500\n",
      "87/87 [==============================] - ETA: 0s - loss: 58.15 - 0s 80us/step - loss: 47.2496 - val_loss: 44.6727\n",
      "Epoch 1299/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.2325 - val_loss: 44.5921\n",
      "Epoch 1300/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.2346 - val_loss: 44.4785\n",
      "Epoch 1301/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.2057 - val_loss: 44.4032\n",
      "Epoch 1302/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.2367 - val_loss: 44.3452\n",
      "Epoch 1303/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2174 - val_loss: 44.3816\n",
      "Epoch 1304/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.2093 - val_loss: 44.4394\n",
      "Epoch 1305/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2129 - val_loss: 44.5065\n",
      "Epoch 1306/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.2076 - val_loss: 44.5298\n",
      "Epoch 1307/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2062 - val_loss: 44.5058\n",
      "Epoch 1308/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.2035 - val_loss: 44.4584\n",
      "Epoch 1309/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.2070 - val_loss: 44.4139\n",
      "Epoch 1310/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2022 - val_loss: 44.3186\n",
      "Epoch 1311/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 47.2209 - val_loss: 44.2279\n",
      "Epoch 1312/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2410 - val_loss: 44.1985\n",
      "Epoch 1313/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2453 - val_loss: 44.1690\n",
      "Epoch 1314/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.2525 - val_loss: 44.1836\n",
      "Epoch 1315/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.2283 - val_loss: 44.2741\n",
      "Epoch 1316/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2019 - val_loss: 44.3614\n",
      "Epoch 1317/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.2049 - val_loss: 44.4378\n",
      "Epoch 1318/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.2098 - val_loss: 44.4696\n",
      "Epoch 1319/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.2158 - val_loss: 44.4615\n",
      "Epoch 1320/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 80us/step - loss: 47.2200 - val_loss: 44.4393\n",
      "Epoch 1321/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2019 - val_loss: 44.4560\n",
      "Epoch 1322/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.2010 - val_loss: 44.4679\n",
      "Epoch 1323/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.2010 - val_loss: 44.4824\n",
      "Epoch 1324/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1949 - val_loss: 44.4658\n",
      "Epoch 1325/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 47.1899 - val_loss: 44.4480\n",
      "Epoch 1326/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.1909 - val_loss: 44.4408\n",
      "Epoch 1327/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1867 - val_loss: 44.4036\n",
      "Epoch 1328/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1971 - val_loss: 44.3729\n",
      "Epoch 1329/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.1886 - val_loss: 44.3974\n",
      "Epoch 1330/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1860 - val_loss: 44.4288\n",
      "Epoch 1331/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1891 - val_loss: 44.4719\n",
      "Epoch 1332/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1801 - val_loss: 44.4806\n",
      "Epoch 1333/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1933 - val_loss: 44.4854\n",
      "Epoch 1334/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.1951 - val_loss: 44.5300\n",
      "Epoch 1335/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1979 - val_loss: 44.5518\n",
      "Epoch 1336/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1933 - val_loss: 44.6116\n",
      "Epoch 1337/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2075 - val_loss: 44.6410\n",
      "Epoch 1338/1500\n",
      "87/87 [==============================] - ETA: 0s - loss: 34.47 - 0s 80us/step - loss: 47.2038 - val_loss: 44.5839\n",
      "Epoch 1339/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1805 - val_loss: 44.4596\n",
      "Epoch 1340/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1829 - val_loss: 44.3451\n",
      "Epoch 1341/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1914 - val_loss: 44.2934\n",
      "Epoch 1342/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.1984 - val_loss: 44.2799\n",
      "Epoch 1343/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2008 - val_loss: 44.2733\n",
      "Epoch 1344/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1875 - val_loss: 44.3337\n",
      "Epoch 1345/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.1930 - val_loss: 44.4099\n",
      "Epoch 1346/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1713 - val_loss: 44.4500\n",
      "Epoch 1347/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1736 - val_loss: 44.5133\n",
      "Epoch 1348/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1768 - val_loss: 44.5768\n",
      "Epoch 1349/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1979 - val_loss: 44.6403\n",
      "Epoch 1350/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.1875 - val_loss: 44.6156\n",
      "Epoch 1351/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1755 - val_loss: 44.5714\n",
      "Epoch 1352/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.1605 - val_loss: 44.4818\n",
      "Epoch 1353/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1744 - val_loss: 44.4115\n",
      "Epoch 1354/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.1848 - val_loss: 44.3773\n",
      "Epoch 1355/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2058 - val_loss: 44.3689\n",
      "Epoch 1356/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.1949 - val_loss: 44.4332\n",
      "Epoch 1357/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1729 - val_loss: 44.5063\n",
      "Epoch 1358/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.1562 - val_loss: 44.5780\n",
      "Epoch 1359/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1576 - val_loss: 44.6650\n",
      "Epoch 1360/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1758 - val_loss: 44.7803\n",
      "Epoch 1361/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2168 - val_loss: 44.8866\n",
      "Epoch 1362/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.2748 - val_loss: 45.0017\n",
      "Epoch 1363/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.3168 - val_loss: 45.0198\n",
      "Epoch 1364/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.3317 - val_loss: 44.9772\n",
      "Epoch 1365/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.3221 - val_loss: 44.9458\n",
      "Epoch 1366/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2976 - val_loss: 44.9151\n",
      "Epoch 1367/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2856 - val_loss: 44.8303\n",
      "Epoch 1368/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.2589 - val_loss: 44.7042\n",
      "Epoch 1369/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 47.2154 - val_loss: 44.5543\n",
      "Epoch 1370/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1858 - val_loss: 44.3918\n",
      "Epoch 1371/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 47.1450 - val_loss: 44.2495\n",
      "Epoch 1372/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.1721 - val_loss: 44.1249\n",
      "Epoch 1373/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 47.1790 - val_loss: 44.0971\n",
      "Epoch 1374/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1699 - val_loss: 44.1279\n",
      "Epoch 1375/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1539 - val_loss: 44.1684\n",
      "Epoch 1376/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1510 - val_loss: 44.2025\n",
      "Epoch 1377/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1711 - val_loss: 44.2193\n",
      "Epoch 1378/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1644 - val_loss: 44.1982\n",
      "Epoch 1379/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1669 - val_loss: 44.1692\n",
      "Epoch 1380/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.1714 - val_loss: 44.1274\n",
      "Epoch 1381/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.1652 - val_loss: 44.1062\n",
      "Epoch 1382/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.1564 - val_loss: 44.0506\n",
      "Epoch 1383/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1672 - val_loss: 43.9787\n",
      "Epoch 1384/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.1503 - val_loss: 43.9474\n",
      "Epoch 1385/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1622 - val_loss: 43.9267\n",
      "Epoch 1386/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1958 - val_loss: 43.9304\n",
      "Epoch 1387/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 47.1729 - val_loss: 44.0451\n",
      "Epoch 1388/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1304 - val_loss: 44.2012\n",
      "Epoch 1389/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.1560 - val_loss: 44.3564\n",
      "Epoch 1390/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1747 - val_loss: 44.4547\n",
      "Epoch 1391/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.1845 - val_loss: 44.4934\n",
      "Epoch 1392/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1914 - val_loss: 44.5259\n",
      "Epoch 1393/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1995 - val_loss: 44.5494\n",
      "Epoch 1394/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.1756 - val_loss: 44.4717\n",
      "Epoch 1395/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.1680 - val_loss: 44.3736\n",
      "Epoch 1396/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1334 - val_loss: 44.3219\n",
      "Epoch 1397/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 80us/step - loss: 47.1412 - val_loss: 44.2625\n",
      "Epoch 1398/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1557 - val_loss: 44.2564\n",
      "Epoch 1399/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1654 - val_loss: 44.2709\n",
      "Epoch 1400/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1636 - val_loss: 44.3231\n",
      "Epoch 1401/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1643 - val_loss: 44.4032\n",
      "Epoch 1402/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.1347 - val_loss: 44.4372\n",
      "Epoch 1403/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.1315 - val_loss: 44.4326\n",
      "Epoch 1404/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1232 - val_loss: 44.3848\n",
      "Epoch 1405/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1271 - val_loss: 44.3788\n",
      "Epoch 1406/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.1253 - val_loss: 44.4204\n",
      "Epoch 1407/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1347 - val_loss: 44.4628\n",
      "Epoch 1408/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1252 - val_loss: 44.4718\n",
      "Epoch 1409/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1156 - val_loss: 44.5314\n",
      "Epoch 1410/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1249 - val_loss: 44.5655\n",
      "Epoch 1411/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 47.1301 - val_loss: 44.5723\n",
      "Epoch 1412/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1217 - val_loss: 44.6155\n",
      "Epoch 1413/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 47.1334 - val_loss: 44.6220\n",
      "Epoch 1414/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.1314 - val_loss: 44.5690\n",
      "Epoch 1415/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.1225 - val_loss: 44.4960\n",
      "Epoch 1416/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1176 - val_loss: 44.4371\n",
      "Epoch 1417/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.1101 - val_loss: 44.4091\n",
      "Epoch 1418/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.1117 - val_loss: 44.3841\n",
      "Epoch 1419/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1154 - val_loss: 44.3924\n",
      "Epoch 1420/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 47.1297 - val_loss: 44.4083\n",
      "Epoch 1421/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1160 - val_loss: 44.3651\n",
      "Epoch 1422/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.1173 - val_loss: 44.3739\n",
      "Epoch 1423/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1140 - val_loss: 44.4245\n",
      "Epoch 1424/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1095 - val_loss: 44.4480\n",
      "Epoch 1425/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.1115 - val_loss: 44.4673\n",
      "Epoch 1426/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.1107 - val_loss: 44.4284\n",
      "Epoch 1427/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.0989 - val_loss: 44.3324\n",
      "Epoch 1428/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.1478 - val_loss: 44.2852\n",
      "Epoch 1429/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1309 - val_loss: 44.3363\n",
      "Epoch 1430/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.1122 - val_loss: 44.4218\n",
      "Epoch 1431/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.0922 - val_loss: 44.5384\n",
      "Epoch 1432/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 47.1277 - val_loss: 44.6260\n",
      "Epoch 1433/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1276 - val_loss: 44.6134\n",
      "Epoch 1434/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1207 - val_loss: 44.5392\n",
      "Epoch 1435/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 47.1036 - val_loss: 44.4552\n",
      "Epoch 1436/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 47.0881 - val_loss: 44.3515\n",
      "Epoch 1437/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 47.1053 - val_loss: 44.2543\n",
      "Epoch 1438/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1079 - val_loss: 44.2146\n",
      "Epoch 1439/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1158 - val_loss: 44.1697\n",
      "Epoch 1440/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1191 - val_loss: 44.1110\n",
      "Epoch 1441/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1266 - val_loss: 44.0993\n",
      "Epoch 1442/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1158 - val_loss: 44.1090\n",
      "Epoch 1443/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.1123 - val_loss: 44.1528\n",
      "Epoch 1444/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 47.0995 - val_loss: 44.1670\n",
      "Epoch 1445/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1074 - val_loss: 44.1525\n",
      "Epoch 1446/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1049 - val_loss: 44.0785\n",
      "Epoch 1447/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.0979 - val_loss: 44.0701\n",
      "Epoch 1448/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.0941 - val_loss: 44.0994\n",
      "Epoch 1449/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.1101 - val_loss: 44.0929\n",
      "Epoch 1450/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.0988 - val_loss: 44.0319\n",
      "Epoch 1451/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 47.0961 - val_loss: 44.0164\n",
      "Epoch 1452/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.0995 - val_loss: 44.0246\n",
      "Epoch 1453/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.0946 - val_loss: 44.0106\n",
      "Epoch 1454/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.0894 - val_loss: 44.0394\n",
      "Epoch 1455/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.0888 - val_loss: 44.0756\n",
      "Epoch 1456/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.0893 - val_loss: 44.1291\n",
      "Epoch 1457/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.0921 - val_loss: 44.1735\n",
      "Epoch 1458/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.0901 - val_loss: 44.2044\n",
      "Epoch 1459/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.0884 - val_loss: 44.2565\n",
      "Epoch 1460/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.0883 - val_loss: 44.3245\n",
      "Epoch 1461/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.1061 - val_loss: 44.3755\n",
      "Epoch 1462/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.0949 - val_loss: 44.3816\n",
      "Epoch 1463/1500\n",
      "87/87 [==============================] - ETA: 0s - loss: 46.71 - 0s 80us/step - loss: 47.0874 - val_loss: 44.4143\n",
      "Epoch 1464/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.0830 - val_loss: 44.3992\n",
      "Epoch 1465/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 47.0808 - val_loss: 44.3585\n",
      "Epoch 1466/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.0715 - val_loss: 44.3397\n",
      "Epoch 1467/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.0746 - val_loss: 44.3544\n",
      "Epoch 1468/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.0768 - val_loss: 44.3847\n",
      "Epoch 1469/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.0668 - val_loss: 44.3877\n",
      "Epoch 1470/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.0674 - val_loss: 44.4262\n",
      "Epoch 1471/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.0763 - val_loss: 44.4890\n",
      "Epoch 1472/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.0739 - val_loss: 44.5033\n",
      "Epoch 1473/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.0793 - val_loss: 44.5056\n",
      "Epoch 1474/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 80us/step - loss: 47.0726 - val_loss: 44.4368\n",
      "Epoch 1475/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.0621 - val_loss: 44.3874\n",
      "Epoch 1476/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.0593 - val_loss: 44.3598\n",
      "Epoch 1477/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.0605 - val_loss: 44.3228\n",
      "Epoch 1478/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.0761 - val_loss: 44.3156\n",
      "Epoch 1479/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.0705 - val_loss: 44.3802\n",
      "Epoch 1480/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.0636 - val_loss: 44.4050\n",
      "Epoch 1481/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 47.0644 - val_loss: 44.4088\n",
      "Epoch 1482/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.0633 - val_loss: 44.4294\n",
      "Epoch 1483/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.0669 - val_loss: 44.4375\n",
      "Epoch 1484/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.0717 - val_loss: 44.4467\n",
      "Epoch 1485/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 47.0723 - val_loss: 44.4689\n",
      "Epoch 1486/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.0797 - val_loss: 44.4745\n",
      "Epoch 1487/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 47.0701 - val_loss: 44.4079\n",
      "Epoch 1488/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.0656 - val_loss: 44.3304\n",
      "Epoch 1489/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.0490 - val_loss: 44.2715\n",
      "Epoch 1490/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.0478 - val_loss: 44.2276\n",
      "Epoch 1491/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.0627 - val_loss: 44.1852\n",
      "Epoch 1492/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.0725 - val_loss: 44.1428\n",
      "Epoch 1493/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 47.0739 - val_loss: 44.1231\n",
      "Epoch 1494/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.0628 - val_loss: 44.1005\n",
      "Epoch 1495/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 47.0550 - val_loss: 44.1094\n",
      "Epoch 1496/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 47.0366 - val_loss: 44.2018\n",
      "Epoch 1497/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 47.0622 - val_loss: 44.3138\n",
      "Epoch 1498/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.1165 - val_loss: 44.3569\n",
      "Epoch 1499/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 47.1040 - val_loss: 44.3111\n",
      "Epoch 1500/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.0978 - val_loss: 44.2739\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, Y_train,\n",
    "          batch_size=64, epochs=1500,\n",
    "          validation_data=(X_val, Y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3dd3Rc1bX48e+eIo16ty1bNnLFNs0YQezQTO8tIYADgQAJeYE0+KWYl6x03iM9JHmBEGoSYkIILST0EqoNruCKhXGRJVnFKlYvs39/nCtZNrJRG82MtD9rzdK95965s+faM3vOPeeeI6qKMcYYA+CLdgDGGGNihyUFY4wx3SwpGGOM6WZJwRhjTDdLCsYYY7pZUjDGGNPNkoIx/SQihSKiIhLow76fFZHXBnscY4aLJQUzoonIFhFpE5HcfcpXeV/IhdGJzJjYZEnBjAYfAAu7VkTkMCApeuEYE7ssKZjR4M/AlT3WrwL+1HMHEckQkT+JSKWIbBWR74iIz9vmF5Gfi0iViGwGzunluXeLSJmI7BCRH4uIv79Bish4EXlCRHaJSLGIfL7HtmNEZJmI1IvIThH5pVceEpG/iEi1iNSKyNsiMra/r21MF0sKZjRYAqSLyCzvy/pS4C/77PNbIAOYApyISyJXe9s+D5wLHAkUARfv89z7gQ5gmrfP6cDnBhDnYqAEGO+9xv+IyCnettuA21Q1HZgKPOSVX+XFPRHIAf4LaB7AaxsDWFIwo0dXbeE0YAOwo2tDj0Rxs6ruVtUtwC+Az3i7XAL8WlW3q+ou4H97PHcscBbwNVVtVNUK4FfAZf0JTkQmAscB31LVFlVdBdzVI4Z2YJqI5Kpqg6ou6VGeA0xT1U5VXa6q9f15bWN6sqRgRos/A58GPss+l46AXCAB2NqjbCswwVseD2zfZ1uXg4AgUOZdvqkF/gCM6Wd844Fdqrp7PzFcC8wANniXiM7t8b6eAR4UkVIR+amIBPv52sZ0s6RgRgVV3YprcD4beGSfzVW4X9wH9SibxJ7aRBnu8kzPbV22A61Arqpmeo90VT2knyGWAtkiktZbDKq6SVUX4pLNT4CHRSRFVdtV9QeqOhv4OO4y15UYM0CWFMxoci1wsqo29ixU1U7cNfpbRCRNRA4CbmJPu8NDwFdEpEBEsoBFPZ5bBjwL/EJE0kXEJyJTReTE/gSmqtuBN4D/9RqPD/fifQBARK4QkTxVDQO13tM6ReQkETnMuwRWj0tunf15bWN6sqRgRg1VfV9Vl+1n85eBRmAz8BrwV+Aeb9sfcZdoVgMr+HBN40rc5ad1QA3wMJA/gBAXAoW4WsOjwPdU9Tlv25nAWhFpwDU6X6aqLcA47/XqgfXAf/hwI7oxfSY2yY4xxpguVlMwxhjTzZKCMcaYbpYUjDHGdLOkYIwxpltcD9mbm5urhYWF0Q7DGGPiyvLly6tUNa+3bXGdFAoLC1m2bH89DI0xxvRGRLbub5tdPjLGGNPNkoIxxphulhSMMcZ0i+s2BWOM6Y/29nZKSkpoaWmJdijDIhQKUVBQQDDY94FzLSkYY0aNkpIS0tLSKCwsRESiHU5EqSrV1dWUlJQwefLkPj/PLh8ZY0aNlpYWcnJyRnxCABARcnJy+l0rsqRgjBlVRkNC6DKQ9zo6k0L1+/Dij2Hzf6DdprM1xpguozMplK6EV38Jfzofbp0E958Pqx+0BGGMiZjq6mrmzJnDnDlzGDduHBMmTOheb2tr69Mxrr76ajZu3BjROON6PoWioiId8B3NLfWw7U3Y8iqsfxJqPoD0CXDK9+DwS2AUVTGNGS3Wr1/PrFmzoh0G3//+90lNTeXrX//6XuWqiqri8w3d7/Xe3rOILFfVot72H501BYBQOsw4A07/MXxlJVz5OKSOhUevg0e/AO2jo8uaMSa6iouLOfTQQ/mv//ov5s6dS1lZGddddx1FRUUccsgh/PCHP+ze97jjjmPVqlV0dHSQmZnJokWLOOKII5g/fz4VFRVDEo91SQVXK5iyAD53Arz6C3jpFmipg0v+DIGEaEdnjImAH/xzLetK64f0mLPHp/O98w7p9/PWrVvHvffeyx133AHArbfeSnZ2Nh0dHZx00klcfPHFzJ49e6/n1NXVceKJJ3Lrrbdy0003cc8997Bo0aLeDt8vo7em0BufD078BpzzC3jvaXjqm9GOyBgzCkydOpWjjz66e33x4sXMnTuXuXPnsn79etatW/eh5yQlJXHWWWcBcNRRR7Fly5YhicVqCr05+lqo3Qav/xqmngSzL4h2RMaYITaQX/SRkpKS0r28adMmbrvtNt566y0yMzO54oorer3XICFhz1UMv99PR0fHkMQyKmsKrR2dNLR+xAk8+TuQfwQ89S1o3T08gRljRr36+nrS0tJIT0+nrKyMZ555Zlhff1TWFF7bVMUX/rycuZOyOHZaLqfMGsMh49P3vtHDH4SzfwF3nwqv3+aShDHGRNjcuXOZPXs2hx56KFOmTOHYY48d1tePWJdUEbkHOBeoUNVDvbI5wB1ACOgArlfVt8R9G98GnA00AZ9V1RUf9RoD7ZJaXLGbh5fv4PXiKtaU1qEKs/LT+eKCqZx3eP7eyeGhK+H9l+Br70JSZr9fyxgTO2KlS+pwiqUuqfcBZ+5T9lPgB6o6B/iutw5wFjDde1wH3B7BuJg2Jo1FZ83kn18+jhXfOY0fXXgo4bDylcUrueQPb1Ja2+MmtuO/Dq318PZdkQzJGGNiQsSSgqq+AuzatxhI95YzgFJv+QLgT+osATJFJD9SsfWUlZLAZ+YdxL+/ejw/+eRhrCut59zfvsb6Mq+rWv7hMPkEWHE/hMPDEZIxxkTNcDc0fw34mYhsB34O3OyVTwC299ivxCv7EBG5TkSWiciyysrKIQvM7xMuPXoSj3/pOBL8Pq64aylbqhrdxrlXud5Im18astczxphYNNxJ4YvAjao6EbgRuNsr721MiV4bO1T1TlUtUtWivLy8IQ9w2phU/vr5j9ERVr60eAWtHZ0w6zxIynLjIxljzAg23EnhKuARb/nvwDHecgkwscd+Bey5tDTspuSl8vNPHcGaHfXc9vwmCCTCwefAe89AR98GrjLGmHg03EmhFDjRWz4Z2OQtPwFcKc48oE5Vy4Y5tr2cNnssn5g7gbte/cBdRpp1HrTWwQevRDMsY4yJqIglBRFZDLwJHCwiJSJyLfB54Bcishr4H1xPI4B/A5uBYuCPwPWRiqs/Fp05k4Bf+NXz77mxkRJSYeO/oh2WMSZOLViw4EM3o/3617/m+uv3/5WXmpoa6bD2EsneRwtVNV9Vg6paoKp3q+prqnqUqh6hqh9T1eXevqqqN6jqVFU9TFUHOB720BqTHuLyj03in6tL2VYfhsLjYPPL0Q7LGBOnFi5cyIMP7t02+eCDD7Jw4cIoRfRho3KYi/743PFTCPh83PP6B662sGuz64lkjDH9dPHFF/Pkk0/S2toKwJYtWygtLWXOnDmccsopzJ07l8MOO4zHH388ajGOymEu+mNseojTDxnLoyt3cPPnjycR3DSecz8T7dCMMYPx1CIof3dojznuMDjr1v1uzsnJ4ZhjjuHpp5/mggsu4MEHH+TSSy8lKSmJRx99lPT0dKqqqpg3bx7nn39+VOaTtppCH1x69ETqmtt5tiILUsa42dqMMWYAel5C6rp0pKr893//N4cffjinnnoqO3bsYOfOnVGJz2oKfXDs1FzyM0I88U4Z5008BkpiosnDGDMYB/hFH0kXXnghN910EytWrKC5uZm5c+dy3333UVlZyfLlywkGgxQWFvY6XPZwsJpCH/h8wumzx/Lqpkra84+CXe9D074jeBhjzEdLTU1lwYIFXHPNNd0NzHV1dYwZM4ZgMMhLL73E1q1boxafJYU+OuOQcbS0h1kRnuYKSt6ObkDGmLi1cOFCVq9ezWWXXQbA5ZdfzrJlyygqKuKBBx5g5syZUYvNLh/10TGTs8lICvJEZQ4fE59LCjPOiHZYxpg4dNFFF9Fz2oLc3FzefPPNXvdtaGgYrrAAqyn0WcDvY/6UHF7e3ITmzYSyd6IdkjHGDDlLCv1w7LQcdtQ205Q1E3aujXY4xhgz5Cwp9MP8qbkAbOIgqC+B5pooR2SM6a9IzTYZiwbyXi0p9MPUvBTGpCWytMmb/2fnuugGZIzpl1AoRHV19ahIDKpKdXU1oVCoX8+zhuZ+EBHmTMzkufJcvgDuElLh8E6qbYwZuIKCAkpKShjKCbpiWSgUoqCgoF/PsaTQT3MmZfLTdeWEs7Lx7VwT7XCMMf0QDAaZPHlytMOIaXb5qJ/mTMwEhLq0GdbYbIwZcSwp9NPhBZmIwDZ/AVRvglFwbdIYM3pYUuin1MQAU/NSWd82BlrqoKk62iEZY8yQsaQwADPHpbG8IdutVBdHNxhjjBlClhQGYFZ+Om/VW1Iwxow8kZyj+R4RqRCRNfuUf1lENorIWhH5aY/ym0Wk2NsW04MKzRyXRonmoRKA6vejHY4xxgyZSHZJvQ/4HfCnrgIROQm4ADhcVVtFZIxXPhu4DDgEGA88LyIzVLUzgvEN2Mz8dDrxsztpAulWUzDGjCARqymo6ivAvpMOfBG4VVVbvX0qvPILgAdVtVVVPwCKgWMiFdtgjc8IkRYKUBqYYDUFY8yIMtxtCjOA40VkqYj8R0SO9sonANt77FfilcUkEWFqXirvd45zE+6Ew9EOyRhjhsRwJ4UAkAXMA74BPCRuZureZqfu9QYAEblORJaJyLJo3qo+JS+FdS050NECDdGZS9UYY4bacCeFEuARdd4CwkCuVz6xx34FQGlvB1DVO1W1SFWL8vLyIh7w/kzNS2Vdc6Zbqdt+4J2NMSZODHdSeAw4GUBEZgAJQBXwBHCZiCSKyGRgOvDWMMfWL1NyUyhRLynVbotuMMYYM0Qi1vtIRBYDC4BcESkBvgfcA9zjdVNtA65SN4btWhF5CFgHdAA3xGrPoy5T8lLZoW5+BUsKxpiRImJJQVUX7mfTFfvZ/xbglkjFM9QOykmmWUI0BzJIsstHxpgRwu5oHqBQ0M+EzCSq/GOtpmCMGTEsKQzCxKxkdpALtVZTMMaMDJYUBmFidhIftGe73kc2hLYxZgSwpDAIBVnJvNeaBe1NNoS2MWZEsKQwCBOzk6wHkjFmRLGkMAgTs5Ip1Ry3Ut/rvXbGGBNXLCkMQkFWMjvVm1dhd1l0gzHGmCFgSWEQxqQlstufSRi/JQVjzIhgSWEQfD4hPyuFukAW7C6PdjjGGDNolhQGqSAriQqyraZgjBkRLCkM0oTMJEo7M6HekoIxJv5ZUhikcRkhtrVnoFZTMMaMAJYUBmlceoidmoW01EJ7c7TDMcaYQbGkMEjjMkJUkOVWrLZgjIlzlhQGaVxGiPLuexWsB5IxJr5ZUhikrstHgNUUjDFxz5LCIGUkBakNdA11YUnBGBPfLCkMkoiQkpZNuwShsSLa4RhjzKBYUhgC4zKTqJNMaKyKdijGGDMoEUsKInKPiFSIyJpetn1dRFREcr11EZHfiEixiLwjInMjFVckjEsPUaXp0FgZ7VCMMWZQIllTuA84c99CEZkInAb0nIDgLGC697gOuD2CcQ25sRkhdnamoZYUjDFxLmJJQVVfAXb1sulXwDeBnvNXXgD8SZ0lQKaI5EcqtqE2Lj1EpaYT3m1JwRgT34a1TUFEzgd2qOrqfTZNALb3WC/xyno7xnUiskxEllVWxsaXcG5qIlWajjRV2lzNxpi4NmxJQUSSgW8D3+1tcy9lvX67quqdqlqkqkV5eXlDGeKA5aYmUq3p+DpboXV3tMMxxpgBG86awlRgMrBaRLYABcAKERmHqxlM7LFvARA381vmpSVQreluxdoVjDFxbNiSgqq+q6pjVLVQVQtxiWCuqpYDTwBXer2Q5gF1qho3d4LlpiZSRYZbsW6pxpg4FskuqYuBN4GDRaRERK49wO7/BjYDxcAfgesjFVckZCQFqZOupGA1BWNM/ApE6sCquvAjthf2WFbghkjFEmkiQjg5D9qxu5qNMXHN7mgeIv60XLdgl4+MMXHMksIQyUpLpUFS7PKRMSauWVIYIq5bagY02OUjY0z8sqQwRHLTEqkKp6LNvd3EbYwx8cGSwhDJTU1kl6YSbqyOdijGGDNglhSGSG5qArWaSrjRagrGmPhlSWGI5KUmUkMavpaaaIdijDEDZklhiGR7NQV/Zwu0N0c7HGOMGRBLCkMkOzmBWlLdSpNdQjLGxCdLCkMkMzmBGvWSQrNdQjLGxCdLCkMkIeCjJeiNf2TdUo0xccqSwhDSpCy3YJePjDFxypLCUErKdn+tpmCMiVOWFIZQINUbFM9qCsaYOGVJYQilpqTQTKI1NBtj4pYlhSGUleLuVbCagjEmXllSGELZXrfUsCUFY0ycsqQwhDJTXFLosEHxjDFxKpJzNN8jIhUisqZH2c9EZIOIvCMij4pIZo9tN4tIsYhsFJEzIhVXJGUnJ1BDKmo1BWNMnOpTUhCRqSKS6C0vEJGv9PxC34/7gDP3KXsOOFRVDwfeA272jjkbuAw4xHvO70XE3+d3ESOyUoLUaio+a2g2xsSpvtYU/gF0isg04G5gMvDXAz1BVV8Bdu1T9qyqdnirS4ACb/kC4EFVbVXVD4Bi4Jg+xhYzspITqCGNQGsthMPRDscYY/qtr0kh7H2ZXwT8WlVvBPIH+drXAE95yxOA7T22lXhlHyIi14nIMhFZVlkZW/MhZ3u9j4QwtNZFOxxjjOm3viaFdhFZCFwFPOmVBQf6oiLybaADeKCrqJfdtLfnquqdqlqkqkV5eXkDDSEiMpODewbFs3YFY0wc6mtSuBqYD9yiqh+IyGTgLwN5QRG5CjgXuFxVu774S4CJPXYrAEoHcvxoSgz4aQ6ku5WW2ugGY4wxAxDoy06qug74CoCIZAFpqnprf19MRM4EvgWcqKpNPTY9AfxVRH4JjAemA2/19/ixIBzKgjbsrmZjTFzqa++jl0UkXUSygdXAvd4X+IGesxh4EzhYREpE5Frgd0Aa8JyIrBKROwBUdS3wELAOeBq4QVU7B/yuoinJ65TVZEnBGBN/+lRTADJUtV5EPgfcq6rfE5F3DvQEVV3YS/HdB9j/FuCWPsYTs3zJ2VCH1RSMMXGpr20KARHJBy5hT0Oz6UUgNcctWFIwxsShviaFHwLPAO+r6tsiMgXYFLmw4ld6cojdJFtSMMbEpb42NP8d+HuP9c3AJyMVVDzLSk6gVlNIbd7Vaz9bY4yJZX1taC7wxiqqEJGdIvIPESn46GeOPpnJQWo1hY4Gu0/BGBN/+nr56F5ct9HxuDuN/+mVmX1kJru7mjvt5jVjTBzqa1LIU9V7VbXDe9wHxNbtxDEiKzlIHanWpmCMiUt9TQpVInKFiPi9xxWATRrQi0xvoh1fiyUFY0z86WtSuAbXHbUcKAMuxg19YfaRmRykllSCrXU2UqoxJu70KSmo6jZVPV9V81R1jKpeCHwiwrHFpa7eR0IY2nZHOxxjjOmXwcy8dtOQRTGCZCR5bQpg7QrGmLgzmKRg3fB74fcJbcEMt2JJwRgTZwaTFHqd78BAONQ1KJ51SzXGxJcD3tEsIrvp/ctfgKSIRDQSJGVDC1ZTMMbEnQMmBVVNG65ARhJJzoYaLCkYY+LOYC4fmf1ISM1yC802+5oxJr5YUoiAtJQUGjVkNQVjTNyxpBABmclBakglbA3Nxpg4Y0khArKSE6jTFDoabCQQY0x8saQQAZnJQXZpGuHGymiHYowx/RKxpCAi93jzL6zpUZYtIs+JyCbvb5ZXLiLyGxEpFpF3RGRupOIaDpnJCVSRgVhSMMbEmUjWFO4DztynbBHwgqpOB17w1gHOAqZ7j+uA2yMYV8RlJQep1EwCzVWgdo+fMSZ+RCwpqOorwL4trRcA93vL9wMX9ij/kzpLgEwRyY9UbJGWlZxAlabj72yBtoZoh2OMMX023G0KY1W1DMD7O8YrnwBs77FfiVf2ISJynYgsE5FllZWxeXkmIzlIlXrjHzVURDcYY4zph1hpaO5tcL1er7uo6p2qWqSqRXl5sTn5W1pigF3ijX9kScEYE0eGOyns7Los5P3t+sYsASb22K8AKB3m2IaMiNCSmOtWGi0pGGPix3AnhSeAq7zlq4DHe5Rf6fVCmgfUdV1milcdSV5SsJqCMSaOHHBAvMEQkcXAAiBXREqA7wG3Ag+JyLXANuBT3u7/Bs4GioEmRsBUn76UXMKNgs+SgjEmjkQsKajqwv1sOqWXfRW4IVKxREN6SohKXx5jd22OdijGGNNnsdLQPOJkJidQzESoWB/tUIwxps8sKURITkoC73YUoFUboa0p2uEYY0yfWFKIkDHpIZZ0zEDCHbBjWbTDMcaYPrGkECFj0xNZEZ6BIrD1jWiHY4wxfWJJIULGpYeoJ4WGrJmw5bVoh2OMMX1iSSFCxqaHANiRNR+2vQkt9VGOyBhjPpolhQjJS0sEYG3qfAh3wLt/j3JExhjz0SwpREgo6CcrOchKDoaDjoOnb4a1j0U7LGOMOSBLChE0Nj1E+e52uPTPMO4w+PtV8PC1NvSFMSZmWVKIoIKsJLbtaoTkbLj6KVhwM6x7HG47Ap75tiUHY0zMsaQQQdPGpPFBVSMdnWEIJMCCRXDDUph1Piz5PfxmLrz1RwiHox2qMcYAlhQiasbYVNo7leLKHrOv5UyFT/wBbngbJh4N//463HcOVL8fvUCNMcZjSSGCjpmcDcBrm6o+vDF3GlzxCFx4O1Sshds/Dm/8DsKdwxylMcbsYUkhggqykjl0QjqL39pGONzLRHIiMOfTcP1SmHISPPttuPt02Llu+IM1xhgsKUTc54+fwvuVjTyycsf+d0rPh4WL4RN3wa7N8IcTYPn9wxekMcZ4LClE2DmH5XN0YRbfeexdHl1Zgps6ohcicPin4EvLYPIJ8M+vwKu/gP3tb4wxEWBJIcICfh+/v/woDhmfwY1/W82ldy7h5Y0VvV9OAkjJgU//DQ67BF74obvpzXonGWOGScRmXjN75KUl8tAX5vOXJVv5/cvFfPbet5mSm8Jn5h/ExUcVkBYK7v0EfxAu+gOk5Lquqw074dxfQlJWdN6AMWbUkP1ezojki4rcCHwOUOBd3JzM+cCDQDawAviMqrYd6DhFRUW6bFl8zVXQ1hHmqTVl3Pv6FlZtryUlwc/FRxVw5ccLmZqXuvfOqvD6r12NISkbTv8RHLHQXWoyxpgBEpHlqlrU67bhTgoiMgF4DZitqs0i8hDwb+Bs4BFVfVBE7gBWq+rtBzpWPCaFnlZvr+X+N7bw5DtltHWGOXFGHtccN5kTpuciPb/4y96Bf/0/KHkLJs2Hc34BYw+JXuDGmLgWi0lhCXAEUA88BvwWeAAYp6odIjIf+L6qnnGgY8V7UuhS1dDKX5du489LtlK5u5UZY1P5xhkzOXXWmD3JIRyGVX+B574HLXUw74tw8ncgmBTd4I0xcSemkgKAiHwVuAVoBp4FvgosUdVp3vaJwFOqemgvz70OuA5g0qRJR23dunXY4o601o5O/vVOGb97sZjNVY0cXZjForNmcdRBPdoSmnbBCz+A5ffBxI/BZYtd47QxxvTRgZLCsPc+EpEs4AJgMjAeSAHO6mXXXrOVqt6pqkWqWpSXlxe5QKMgMeDnE3MLeObGE/jxhYfyQVUTn7z9Da6+9y2WbK523VmTs+G82+BT90PpKrj7NHdvgzHGDIFodEk9FfhAVStVtR14BPg4kCkiXb2hCoDSKMQWE4J+H1fMO4j/fGMBN502g3d31HHZnUs45zev8dCy7bS0d8IhF8JVT0DzLrjrNCiJ/8toxpjoi0ZS2AbME5FkcRfMTwHWAS8BF3v7XAU8HoXYYkpKYoCvnDKdV795Mv/7icPoDCvffPgdPn7ri/zsmQ3sypkL1z4Pialw37mwarHd7GaMGZRotSn8ALgU6ABW4rqnTmBPl9SVwBWq2nqg44yUhua+UlXe3FzNfa9v4fn1O0lOCPClk6fx2SNSCD16DWx9HQ6/FM7/LQQSox2uMSZGxVxD81AZbUmhp+KK3fzvvzfwwoYKMpODfGLOOK73P0bu27+A8XNdcph2CuRMs/sajDF7saQwgr35fjV/WbqVZ9eW096pfG3MSq7tfIi0Rq9XVsYkmHqSSxCTT4SkzOgGbIyJOksKo0B1QyuPrNjB4re3sbmykZmJ1Vw9bjMn+t9lbNUSpK0BxAcTilyCmHoKTJgLPn+0QzfGDDNLCqOIqvL2lhr+sbyEZ9eVU9PUTmowzFUTqzk/dT3Tdr+Fv2wloBDKcPM4zDgDpp0GqSOri68xpneWFEapjs4wb32wi6fXlvP0mnIqdrcS9AunT07g8rzNzG1fSWjLi9BQDgiMP9IliOmnQ/4c8NkgusaMRJYUDOGwsnJ7LU+vKeOpNeWU1DTjE/hYYTafPqiWBb6VpG170bvfQSFlDEw/Daad6tokbIRWY0YMSwpmL6rK2tJ6nllbzlNryimuaADgyEmZXDg9kXOS15Jb+jK8/yK01IL4oeBomH6qq0WMO9x6NBkTxywpmAMqrtjNM2t38tSaMtbsqAdgdn46Zx2Sy4V55RRUvYYUPwdlq90TUse6GsS0U2HqydajyZg4Y0nB9Nn2XU0847VBLN9WgypMyUvhrEPHce5kPzMb30KKn9+7FjFpHhx8Nhz2KUgbG+23YIz5CJYUzIBU1LfwzLqdPLOmnDc3V9MZViZmJ3HhnAlcdMRYprRugE3PwnvPws53XYKYdoqbCOjgsyEYivZbMMb0wpKCGbSaxjaeW7+Tf64u5fXiKsIKR0zM5BNHTuC8I8aT3fQBrH4Q3vkb1O+AxAw3aN+cy2HiMdYGYUwMsaRghtTO+haeWFXKIyt3sL6snoBPWHBwHhcdWcApB+cQ2vGGSxDrnoD2RsiZDkde7moQaeOiHb4xo54lBRMxG8rreXTFDh5btYOd9a2khQKcc1g+nzyqgKL8ILLucVjxZ9i+xN1RPe1UKLoGpp9h90EYEyWWFEzEdYaVN9+v5pGVJTy9ppymtk4OnZDO54+fwtmH5ROs2QyrHuYoxhcAABRbSURBVIDVi2F3GWRPgflfgiM/A4GEaIdvzKhiScEMq6a2Dh5bWcpdr21mc2Uj4zNC3HDyNC4pmkiQTlj/BLzxOyhdAZmT4MRvweGXgT/w0Qc3xgyaJQUTFeGw8tLGCn7/8vss31pDYU4yN542g/MOH49PgOIX4MUfQdkqN8T32T93d08bYyLKkoKJKlXlxQ0V/OyZjWwo382s/HQWnTWTE2fkuZniNvwLnv8+VG+C478OJ33b2huMiSBLCiYmhMPKE6tL+eVz77FtVxOXFBXw3fMOITUxAG1N8NQ3YOVfYNb58Ik7IZgU7ZCNGZEOlBTs55gZNj6fcOGRE3j+phO54aSpPLy8hLNve5XlW2sgIRnO/x2cfgus/yc88ClobYh2yMaMOpYUzLBLCPj4xhkz+dsX5hNWZeGdS3h6TZm7we3jX3K1hK1vwN2nw+q/uUtMxphhEZWkICKZIvKwiGwQkfUiMl9EskXkORHZ5P21sZpHuKMLs3nyy8dx6IR0rn9gBX97e5vbcPglcMn90FwDj14HD14OnR3RDdaYUSJaNYXbgKdVdSZwBLAeWAS8oKrTgRe8dTPCZSYn8JfPfYzjpufxrX+8y1+WeHNLzzoPblwLp/0INv4Lnl5kNQZjhsGwJwURSQdOAO4GUNU2Va0FLgDu93a7H7hwuGMz0ZGcEOCuK4s4eeYYvvfEWl7dVOk2+Hxw7FfcTW5v/xGW3hHdQI0ZBaJRU5gCVAL3ishKEblLRFKAsapaBuD9HdPbk0XkOhFZJiLLKisrhy9qE1EJAR+/WXgk0/JSuf6BFWyu7NHIfNoPYea58PTN8J+fuvaG6vejF6wxI9iwd0kVkSJgCXCsqi4VkduAeuDLqprZY78aVT1gu4J1SR15tu9q4vzfvcbY9BCPXn8sSQl+t6G9Gf7xOdjw5J6dZ18AM8+Dxgo49JM22J4xfRRT9ymIyDhgiaoWeuvH49oPpgELVLVMRPKBl1X14AMdy5LCyPTyxgo+e+/bXFJUwE8vPmLPBlWo3OCG5t62FF77FYTb3bZQBhx3o5sqNJgMY2bZjHDG7MeBksKwDzajquUisl1EDlbVjcApwDrvcRVwq/f38eGOzcSGBQeP4UsnTeN3LxVzdGE2nyqa6DaIuC/7MbPcaKvHfB52l4PPD099y90V3SUQglAmzDzHTRmanANjZkKSdWoz5kCickeziMwB7gISgM3A1bj2jYeAScA24FOquutAx7GawsjVGVauuGspK7fX8NgNxzJzXPpHP2l3OVQXQ0s9vPcUNFZD8fPQ2bpnn1nnw1Gfhc52GH+kTR9qRqWYunw0lCwpjGwVu1s45zevkRYK8MSXjnPDYfRX0y6o3Aitu2HbG7D0D9De5LaJ380rPWaW2z77fMg/4sDHM2YEsKRg4tab71dz+V1LOH32OH5/+Vx8vkFO67m73LVLBEJuZrhld0NHC+Ad97CLoeBoSMuHhp2QPgEOPsumEzUjiiUFE9fuenUzP/7Xer588jT+3+kH7HvQf60NoJ2uEfuVn8Gye/bUJLqk5EHuwZA3A3K9x6T5brwmY+JQTDU0G9Nf1x43mU07G/jti8VMyUvhoiMLhu7gial7ls+4BU7/sash7C53XVw3PQfb3oSqTbDmH9BS5/YNJLmG68LjIPMgSMmFWRfYkN8m7llNwcSFto4wV96zlKUf7OLTx0ziY1NyGJ8RYkxaiLEZiSQG/JEPQhUaq6B8tZsgqPxdlzDC3rhMGZOgpRbm3+DaKj54xfV+Su31PkxjosYuH5kRobmtkx/9ax0PLyuhrTO817bc1ETyM0LkZ4QYn5nE+MwQY9ND5KUmkpeWSE5qIhlJQfyDbZPYV9MuaGuEzS95I7p2ukTRJSEV5lzuLkGl5rkaxuTjIX380MZhTD9YUjAjSnNbJ9t2NVFe38LO+hbKalsoq2umtK6FstpmSmubaWzr/NDzRCA9FCQ3NYExaSFyUhPISUkgOyWR7JSg9zeBnNQEslMSyEpO6H8SUYXl97musTPOgLfvgvVPumTRJTEdiq4BFMKdbjlnqtsWDrt9/cEBnx9jPoolBTOqqCr1LR1U7m51j4ZWqhtaqWlqp7apjaqGVnbWt1LT2EZ1Yxt1ze37PVZaYoDcNFfLyEwOkpXsEkmWlzQyk/eUd/0NBfe5lKXq7ovYXQYNFfDSj2HLa3T3eNIwZBS4kWHL34Xyd+CyxS5RdLa5bcYMIUsKxhxAe2eYmqY2djW2savBJYpdjW3UNLVR29ROdWMbtU0uedQ0tVHd0EZTLzWRLokBX68JIzM5gazkIJlJCWQnhslICpITaCZ/9f+RuGs9vm1v7H0g8QECRVe7O7Enfgyyp7iGbX9gz1Di1l3W9JP1PjLmAIJ+H2PSXKN1X7W0d3YnjZqmNuqa2qnpWm5up6axjdpmVzPZVNFArVdL6Qj39iPsFOAU5iWeSygxkapwKp8PPk1CSgYT2rdx+Nt37bV32BekJXUSCS1VaCCR3XOuIzz7IoLZEwmUr8SfWUAo22oXZmCspmDMMFFVGts6XcJoaqe2uY2apnbqmtr2JJSmdkSEsrpmSmqaaW7vRJt20djp52jfRsZILdOklKlSSoVmMkl2crx/DQDt6iconVRrGj/TK8jxN5PsD9MYyGCHfxJndzxHUriZ11NPY1P6fGa0ryetuYTN484hMcFPKOAnMegjwe8jIeAjMeAj6PcR8AtBn/fX7yPoF1o7wrS2h0lPCpIeCpCcGPCeJyT4/fh8EPD58PsERUnw732sQd+EaAbFagrGxAARITUxQGpigInZfX+eqtLc3klTWyd1ze00t3XS0t5JyCt7sfJdkitWkdKwhQ71UVjxAre23g4KdHgPoFVCtPiSOKHmVUrr8hkfLgPg9t2lPKSnoh2t+Dua8YXbqNUUKonc4IEB354EE/QSRjDglv0iKJAU9JMQ2LNPYsAlq4Tu/buWhYDf3R8iuJqfqpIY9BPwyV5tPCLgE/Eebl0Q6C53f7v2C6sSViXg83VvB3dqBfD5BL8IjW0d7Kht7u7tBm78rrBC0C8kBf34fHuO3/Vo7ehEBPw+HwGfdMfj83lxeDF3/Xb3+YTEgI+mtk5SEwOMz0wa8n8bqykYM9K0NrjxnrIOcsN5lK2Cmq1uuI6EVFhxP2z4F4w7DHauhfdf6PUw4YyDaDvoBNpyZ6PNNYTVh792M+Gw0lxwLIHkLBo7/ZTnzqOpPUxbh9LWGaa9I0xnWOkIKx3hMD4R2jvD3kP3Wm7rcMsdXnlbp3uuCDS1de71nJZ2t2/Xc7qWO8JuO0BY3ZfxaPDFBVP51pkzB/Rca2g2xvSuo83NgV1f6hJIMBkCie6u7i2vuRvwWuv37B9Mcd1lW2r3lIUyYewh7ia9cAe0NbmhQtqbXc+p3BmuJ1VSNiRnu/Gk0vJdY3kEdH2ntXaECavS3NaJiKCqKLhf/2H3V739Vd2vccX9ug+resdxv+7DqnSG3XbVPb/eXW1ASQz4mZCVRHVDK1UNrYi4GoTf52oDre1hl7C8mod6x0sIuBpO2EuiLqG51+iOA7prDJ1hpbU9TCjBz+z8dKaNSe39JHwESwrGmIEJh6GpyiULXwD8Ca68Yq0bWbauBLa8CpXvuf18QTcmVDDZ7Vu7FWq27Lnru4v4XGJIn+DVaBLh/Zdcgpl6Ehx0rDteIOSWty+FivWuF1b9Dnd/RyARpizYe2Tb9mbImOCWu769h8POta77cP6cuOgNZknBGBM9ne1Qt92NG9VYDfUlLpnU7XDlNVugqdpNnNRS5+4I72z76OOKz93jse/6+LmQO93dNDjtZGiodLWbtHz3OtlTXE2ncoNbbq5xsWQUuDGsara4Gfx8AZg0zyW2hgp3jPYWqN0GY2e749bvgISUPdPEHnw2nPJdSExzCeyVn7vZAVPHufdcuw1QN5dHZ5tbD3fAUVe7108d6xLb1tegYoPb1rob0vNdedo4mFAEFetg4jGQVTigfxJLCsaY2NbzV317M5SudLWGhnLYuc7dozH+SPcFnTbOSwbiLnGVv+vWtROCSbDucShdBXkHuy/+MbO9S2KVEEqH2u3uCzs9HxorwZ/o9q3b7sa2Ss6GstVeXF7SScxwX+6+ICRnucQRTIGUHJcw5n3Rtde89D977l4PhLxh2T0JaW5Sp/YWlxjBvQd/wt4TQe1FXDLqLUl+/MtuAMcBsKRgjBldOtvdl2lb08CGOG9vcTWFbW+4Gkbu9L2315W4xJKQ4i5l+bweTjtWuETU2uB+zR/2KciZ5hJK153pXXe4oy4B+gKw8d9ueXeZO+aEo1yNQMQlptYGl6zK34XSFa6GkD0VMicO6PRYUjDGGNPtQEkhaoO/i4hfRFaKyJPe+mQRWSoim0TkbyKSEK3YjDFmtIrmjCBfBdb3WP8J8CtVnQ7UANdGJSpjjBnFopIURKQAOAe4y1sX4GTgYW+X+4ELoxGbMcaMZtGqKfwa+CbQ1Z8sB6hV1a7OzCXAhN6eKCLXicgyEVlWWVkZ+UiNMWYUGfakICLnAhWqurxncS+79toCrqp3qmqRqhbl5eVFJEZjjBmtojEg3rHA+SJyNhAC0nE1h0wRCXi1hQKgNAqxGWPMqDbsNQVVvVlVC1S1ELgMeFFVLwdeAi72drsKeHy4YzPGmNEumr2P9vUt4CYRKca1Mdwd5XiMMWbUieub10SkEtg6wKfnAlVDGE4kWIyDF+vxQezHGOvxgcXYXwepaq+NsnGdFAZDRJbt746+WGExDl6sxwexH2OsxwcW41CKpctHxhhjosySgjHGmG6jOSncGe0A+sBiHLxYjw9iP8ZYjw8sxiEzatsUjDHGfNhorikYY4zZhyUFY4wx3UZlUhCRM0Vko4gUi8iiKMUwUUReEpH1IrJWRL7qlWeLyHPevBLPiUiWVy4i8hsv5ndEZO4wxtqnuS9EJNFbL/a2Fw5TfJki8rCIbPDO5/xYOo8icqP3b7xGRBaLSCja51BE7hGRChFZ06Os3+dMRK7y9t8kIlcNQ4w/8/6d3xGRR0Uks8e2m70YN4rIGT3KI/J57y2+Htu+LiIqIrneelTO4YCo6qh6AH7gfWAKkACsBmZHIY58YK63nAa8B8wGfgos8soXAT/xls8GnsINHjgPWDqMsd4E/BV40lt/CLjMW74D+KK3fD1wh7d8GfC3YYrvfuBz3nICkBkr5xE32u8HQFKPc/fZaJ9D4ARgLrCmR1m/zhmQDWz2/mZ5y1kRjvF0IOAt/6RHjLO9z3IiMNn7jPsj+XnvLT6vfCLwDO7G2txonsMBva9ovnhU3jDMB57psX4zcHMMxPU4cBqwEcj3yvKBjd7yH4CFPfbv3i/CcRUAL+Dmu3jS+09d1eOD2X0+vQ/CfG854O0nEY4v3fvSlX3KY+I84pLCdu9DH/DO4RmxcA6Bwn2+cPt1zoCFwB96lO+1XyRi3GfbRcAD3vJen+Ou8xjpz3tv8eHmhTkC2MKepBC1c9jfx2i8fNT1Ie2y37kbhot3ieBIYCkwVlXLALy/Y7zdohV3f+a+6I7R217n7R9JU4BK4F7vEtddIpJCjJxHVd0B/BzYBpThzslyYuscdunvOYv2Z+ka3K9vDhDLsMYoIucDO1R19T6bYiK+vhiNSaHPczcMBxFJBf4BfE1V6w+0ay9lEY1b+j/3RTTObQBXhb9dVY8EGnGXPvZnWGP0rstfgLukMR5IAc46QAwx9f/Ts7+YohariHwb6AAe6CraTyzDFqOIJAPfBr7b2+b9xBFz/96jMSmU4K75dYna3A0iEsQlhAdU9RGveKeI5Hvb84EKrzwacXfNfbEFeBB3Cal77ote4uiO0dueAeyKcIwlQImqLvXWH8YliVg5j6cCH6hqpaq2A48AHye2zmGX/p6zqHyWvMbYc4HL1bvmEiMxTsUl/9XeZ6YAWCEi42Ikvj4ZjUnhbWC61/sjAdeY98RwByEighsefL2q/rLHpidw80nA3vNKPAFc6fVimAfUdVX1I0X7P/dFz9gv9vaP6K8eVS0HtovIwV7RKcA6Yuc8bgPmiUiy92/eFV/MnMMe+nvOngFOF5Esr0Z0ulcWMSJyJm6Y/fNVtWmf2C/zem9NBqYDbzGMn3dVfVdVx6hqofeZKcF1Jiknhs7hR4pmg0a0HrieAO/heiV8O0oxHIerJr4DrPIeZ+OuH78AbPL+Znv7C/B/XszvAkXDHO8C9vQ+moL7wBUDfwcSvfKQt17sbZ8yTLHNAZZ55/IxXC+OmDmPwA+ADcAa4M+4HjJRPYfAYlwbRzvuy+vagZwz3HX9Yu9x9TDEWIy7Bt/1mbmjx/7f9mLcCJzVozwin/fe4ttn+xb2NDRH5RwO5GHDXBhjjOk2Gi8fGWOM2Q9LCsYYY7pZUjDGGNPNkoIxxphulhSMMcZ0s6RgzAGISKeIrOrxGMpRNgt7G2HTmGgKfPQuxoxqzao6J9pBGDNcrKZgzACIyBYR+YmIvOU9pnnlB4nIC96Y+S+IyCSvfKw3/v9q7/Fx71B+EfmjuPkWnhWRpKi9KWOwpGDMR0na5/LRpT221avqMcDvcGNC4S3/SVUPxw3W9huv/DfAf1T1CNzYTGu98unA/6nqIUAt8MkIvx9jDsjuaDbmAESkQVVTeynfApysqpu9gQ3LVTVHRKpwcxK0e+VlqporIpVAgaq29jhGIfCcqk731r8FBFX1x5F/Z8b0zmoKxgyc7md5f/v0prXHcifWzmeizJKCMQN3aY+/b3rLb+BG4gS4HHjNW34B+CJ0z3mdPlxBGtMf9qvEmANLEpFVPdafVtWubqmJIrIU9+NqoVf2FeAeEfkGbka4q73yrwJ3isi1uBrBF3EjbBoTU6xNwZgB8NoUilS1KtqxGDOU7PKRMcaYblZTMMYY081qCsYYY7pZUjDGGNPNkoIxxphulhSMMcZ0s6RgjDGm2/8Hec7Bu6Ig8lMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
